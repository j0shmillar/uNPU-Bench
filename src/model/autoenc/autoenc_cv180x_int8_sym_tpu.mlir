#loc = loc(unknown)
module @autoenc attributes {module.FLOPs = 405184 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv180x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.mode = "INT8", module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_LOWERED", module.top_run_mode = "STATIC", module.weight_file = "autoenc_tpu_lowered_cv180x_int8_sym_weight.npz"} {
  func.func @main(%arg0: tensor<1x256x3xf32> loc(unknown)) -> tensor<1x256x3xf32> {
    %0 = "top.None"() : () -> none loc(#loc)
    %1 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", scale = [1.000000e+00, 1.000000e+00, 1.000000e+00]} : (tensor<1x256x3xf32>) -> tensor<1x256x3x!quant.calibrated<f32<-3.4038111999999998:3.4038111999999998>>> loc(#loc1)
    %2 = "tpu.GenericCpu"(%1) {cpu_op_name = "quant", param = {from = "FP32", scale = 37.3111191 : f32, to = "INT8"}} : (tensor<1x256x3x!quant.calibrated<f32<-3.4038111999999998:3.4038111999999998>>>) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.026801662992125982>> loc(#loc2)
    %3 = "top.Weight"() : () -> tensor<128x256x1x1xsi8> loc(#loc3)
    %4 = "top.Weight"() : () -> tensor<1x128x1x1xi32> loc(#loc4)
    %5 = "tpu.Conv2D"(%2, %3, %4) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multiplier = [1722247296, 1697786880, 1567307648, 1906007424, 1694451456, 1632996736, 1813329536, 1980310912, 2040155904, 2144367872, 1714515456, 1627196288, 1769987328, 1772765696, 1774498176, 1976210048, 1556742528, 1639757952, 1099273216, 1736087680, 1968546944, 1608263040, 1860333696, 1578347520, 1810820864, 1898342272, 1719532160, 1698814080, 1836621696, 1835569152, 1875195136, 1682174976, 1885918976, 1542783744, 1799603840, 1867273344, 1668590720, 1822413056, 2044073984, 1717805568, 1798591488, 1508685824, 1820772608, 1730574592, 1722785536, 1840230656, 1762308992, 1440368640, 1991225728, 1559509632, 1774208256, 1821023232, 1890915328, 1890180480, 1955643904, 1774551680, 1844805376, 1681514368, 1970439424, 1897505280, 1597482496, 1855190400, 1902627328, 1958785792, 1747632256, 1773469952, 1744566528, 2130624768, 1694792192, 1414029696, 1935249152, 1750038784, 1846153728, 1813642624, 1872906240, 1720909056, 1673178112, 1735222528, 1784301184, 1789349120, 1800276608, 1669434880, 1722113792, 2088697344, 1707185152, 2032579584, 1796783744, 1906160896, 1848735488, 1820334592, 1762883840, 1896139520, 1769203840, 1664656512, 1900099200, 1641364352, 1776034816, 1776160896, 1758534656, 1779004672, 1815041664, 1686207232, 2006131456, 1739268096, 2059398784, 1631308544, 1745850240, 1796522368, 1827819648, 1731675648, 1952132992, 1804451840, 1928168320, 1778360704, 1562680448, 1574504832, 1663707904, 1607076736, 1670378240, 1760875776, 1759215744, 1673987584, 1841732224, 1978775424, 1668079104, 1923219840, 1727591936, 1848754944], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, rshift = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x3x!quant.uniform<i8:f32, 0.026801662992125982>>, tensor<128x256x1x1xsi8>, tensor<1x128x1x1xi32>) -> tensor<1x128x3x!quant.uniform<i8:f32, 0.066253759842519688>> loc(#loc5)
    %6 = "top.Weight"() : () -> tensor<64x128x3x1xsi8> loc(#loc6)
    %7 = "top.Weight"() : () -> tensor<1x64x1x1xi32> loc(#loc7)
    %8 = "tpu.Conv2D"(%5, %6, %7) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 1], kernel_zp = 0 : i64, multiplier = [1220200192, 1227484160, 1132529536, 1655222528, 1944164480, 2007080704, 1230766208, 1756442624, 1873104896, 1097805056, 1880891648, 1707755008, 1753793408, 1095770368, 1858111232, 2049045504, 1768160896, 1103493504, 1570612352, 1977687168, 1585399936, 1298793344, 1924072192, 1857881600, 1129896704, 1954937344, 1683358848, 1201840128, 1091876352, 2042033536, 1958539136, 1215154048, 1971634304, 1658494336, 1150735232, 2133849728, 1889960064, 1696672000, 1900964864, 2144482304, 1402679424, 1981879296, 1794994944, 1664481536, 1688631040, 1969029760, 1191464064, 1151887872, 1859078016, 1073974528, 1234578560, 2050558720, 1084799360, 1651044224, 1445574784, 2131227136, 1107522560, 1878807808, 1127041792, 1095879936, 1693016832, 1189549184, 1882416128, 1759411584], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, rshift = [8, 8, 8, 9, 9, 9, 8, 9, 9, 8, 9, 9, 9, 8, 9, 9, 9, 8, 9, 9, 9, 8, 9, 9, 8, 9, 9, 8, 8, 9, 9, 8, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 9, 8, 8, 9, 8, 9, 9, 9, 8, 9, 8, 8, 9, 8, 9, 9], strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x3x!quant.uniform<i8:f32, 0.066253759842519688>>, tensor<64x128x3x1xsi8>, tensor<1x64x1x1xi32>) -> tensor<1x64x1x!quant.uniform<i8:f32, 0.030081116535433071>> loc(#loc8)
    %9 = "tpu.Reshape"(%8) {flatten_start_dim = -1 : i64, shape = [1, 64]} : (tensor<1x64x1x!quant.uniform<i8:f32, 0.030081116535433071>>) -> tensor<1x64x!quant.uniform<i8:f32, 0.030081116535433071>> loc(#loc9)
    %10 = "top.Weight"() : () -> tensor<64x32xsi8> loc(#loc10)
    %11 = "top.Weight"() : () -> tensor<32xsi32> loc(#loc11)
    %12 = "tpu.MatMul"(%9, %10, %11, %0, %0) {do_relu = true, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1177509376], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [6]} : (tensor<1x64x!quant.uniform<i8:f32, 0.030081116535433071>>, tensor<64x32xsi8>, tensor<32xsi32>, none, none) -> tensor<1x32x!quant.uniform<i8:f32, 0.022917204724409449>> loc(#loc12)
    %13 = "top.Weight"() : () -> tensor<32x4xsi8> loc(#loc13)
    %14 = "tpu.MatMul"(%12, %13, %0, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1834483072], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [6]} : (tensor<1x32x!quant.uniform<i8:f32, 0.022917204724409449>>, tensor<32x4xsi8>, none, none, none) -> tensor<1x4x!quant.uniform<i8:f32, 0.025377328346456694>> loc(#loc14)
    %15 = "top.Weight"() : () -> tensor<4x32xsi8> loc(#loc15)
    %16 = "top.Weight"() : () -> tensor<32xsi32> loc(#loc16)
    %17 = "tpu.MatMul"(%14, %15, %16, %0, %0) {do_relu = true, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1615084288], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [7]} : (tensor<1x4x!quant.uniform<i8:f32, 0.025377328346456694>>, tensor<4x32xsi8>, tensor<32xsi32>, none, none) -> tensor<1x32x!quant.uniform<i8:f32, 0.019771251181102362>> loc(#loc17)
    %18 = "top.Weight"() : () -> tensor<32x96xsi8> loc(#loc18)
    %19 = "top.Weight"() : () -> tensor<96xsi32> loc(#loc19)
    %20 = "tpu.MatMul"(%17, %18, %19, %0, %0) {do_relu = true, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1980202752], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [7]} : (tensor<1x32x!quant.uniform<i8:f32, 0.019771251181102362>>, tensor<32x96xsi8>, tensor<96xsi32>, none, none) -> tensor<1x96x!quant.uniform<i8:f32, 0.012892165354330709>> loc(#loc20)
    %21 = "top.Weight"() : () -> tensor<96x768xsi8> loc(#loc21)
    %22 = "tpu.MatMul"(%20, %21, %0, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1484088960], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [8]} : (tensor<1x96x!quant.uniform<i8:f32, 0.012892165354330709>>, tensor<96x768xsi8>, none, none, none) -> tensor<1x768x!quant.uniform<i8:f32, 0.008232239370078739>> loc(#loc22)
    %23 = "tpu.Reshape"(%22) {flatten_start_dim = -1 : i64, shape = [1, 256, 3]} : (tensor<1x768x!quant.uniform<i8:f32, 0.008232239370078739>>) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>> loc(#loc23)
    %24 = "tpu.Cast"(%23) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>>) -> tensor<1x256x3xf32> loc(#loc24)
    return %24 : tensor<1x256x3xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("input")
#loc2 = loc("input/en_conv1/activate/Relu_output_0_Relu_si8")
#loc3 = loc("/en_conv1/activate/Relu_output_0_Relu_filter_i8")
#loc4 = loc("/en_conv1/activate/Relu_output_0_Relu_bias_int32")
#loc5 = loc("/en_conv1/activate/Relu_output_0_Relu")
#loc6 = loc("/en_conv2/activate/Relu_output_0_Relu_filter_i8")
#loc7 = loc("/en_conv2/activate/Relu_output_0_Relu_bias_int32")
#loc8 = loc("/en_conv2/activate/Relu_output_0_Relu")
#loc9 = loc("/Reshape_output_0_Reshape")
#loc10 = loc("/en_lin1/activate/Relu_output_0_Relu_filter_i8")
#loc11 = loc("/en_lin1/activate/Relu_output_0_Relu_bias_int32")
#loc12 = loc("/en_lin1/activate/Relu_output_0_Relu")
#loc13 = loc("/en_lin2/MatMul_output_0_MatMul_filter_i8")
#loc14 = loc("/en_lin2/MatMul_output_0_MatMul")
#loc15 = loc("/de_lin1/activate/Relu_output_0_Relu_filter_i8")
#loc16 = loc("/de_lin1/activate/Relu_output_0_Relu_bias_int32")
#loc17 = loc("/de_lin1/activate/Relu_output_0_Relu")
#loc18 = loc("/de_lin2/activate/Relu_output_0_Relu_filter_i8")
#loc19 = loc("/de_lin2/activate/Relu_output_0_Relu_bias_int32")
#loc20 = loc("/de_lin2/activate/Relu_output_0_Relu")
#loc21 = loc("/out_lin/MatMul_output_0_MatMul_filter_i8")
#loc22 = loc("/out_lin/MatMul_output_0_MatMul")
#loc23 = loc("output_Reshape")
#loc24 = loc("output_Reshape_f32")

