#loc = loc(unknown)
#loc1 = loc("input")
module @autoenc attributes {module.FLOPs = 405184 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv180x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["input"], module.mode = "INT8", module.outputs = ["output_Reshape"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "autoenc_tpu_addressed_cv180x_int8_sym_weight.npz"} {
  module @autoenc attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 138816 : i64, module.device_id = 0 : i64, module.neuron_size = 448 : i64, module.private_size = 768 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x256x3xf32> loc(unknown)) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", scale = [1.000000e+00, 1.000000e+00, 1.000000e+00]} : (tensor<1x256x3xf32>) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.026801662992125982>, 3298534883328 : i64> loc(#loc1)
      %1 = call @subfunc_0(%0) : (tensor<1x256x3x!quant.uniform<i8:f32, 0.026801662992125982>, 3298534883328 : i64>) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> loc(#loc)
      return %1 : tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x256x3x!quant.uniform<i8:f32, 0.026801662992125982>, 3298534883328 : i64> loc("input")) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099511765440 : i64> loc(#loc2)
      %2 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x256xsi8, 1099511658944 : i64> loc(#loc3)
      %3 = "tpu.Conv2D"(%arg0, %2, %1) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x3x!quant.uniform<i8:f32, 0.026801662992125982>, 3298534883328 : i64>, tensor<1x128x1x256xsi8, 1099511658944 : i64>, tensor<1x128x1x9xi8, 1099511765440 : i64>) -> tensor<1x128x3x!quant.uniform<i8:f32, 0.066253759842519688>, 0 : i64> loc(#loc4)
      %4 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099511658368 : i64> loc(#loc5)
      %5 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x3x128xsi8, 1099511633792 : i64> loc(#loc6)
      %6 = "tpu.Conv2D"(%3, %5, %4) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x3x!quant.uniform<i8:f32, 0.066253759842519688>, 0 : i64>, tensor<1x64x3x128xsi8, 1099511633792 : i64>, tensor<1x64x1x9xi8, 1099511658368 : i64>) -> tensor<1x64x1x!quant.uniform<i8:f32, 0.030081116535433071>, 384 : i64> loc(#loc7)
      %7 = "top.Weight"() {do_compress = true} : () -> tensor<64x32xsi8, 1099511631744 : i64> loc(#loc8)
      %8 = "top.Weight"() {do_compress = true} : () -> tensor<32xsi32, 1099511631232 : i64> loc(#loc9)
      %9 = "top.Weight"() {do_compress = true} : () -> tensor<32x4xsi8, 1099511631104 : i64> loc(#loc10)
      %10 = "top.Weight"() {do_compress = true} : () -> tensor<4x32xsi8, 1099511630976 : i64> loc(#loc11)
      %11 = "top.Weight"() {do_compress = true} : () -> tensor<32xsi32, 1099511630848 : i64> loc(#loc12)
      %12 = "top.Weight"() {do_compress = true} : () -> tensor<32x96xsi8, 1099511627776 : i64> loc(#loc13)
      %13 = "top.Weight"() {do_compress = true} : () -> tensor<96xsi32, 1099511631360 : i64> loc(#loc14)
      %14 = "top.Weight"() {do_compress = true} : () -> tensor<96x768xsi8, 1099511691712 : i64> loc(#loc15)
      %15 = "tpu.Reshape"(%6) {flatten_start_dim = -1 : i64, shape = [1, 64]} : (tensor<1x64x1x!quant.uniform<i8:f32, 0.030081116535433071>, 384 : i64>) -> tensor<1x64x!quant.uniform<i8:f32, 0.030081116535433071>, 384 : i64> loc(#loc16)
      %16 = "tpu.MatMul"(%15, %7, %8, %0, %0) {do_relu = true, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1177509376], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [6]} : (tensor<1x64x!quant.uniform<i8:f32, 0.030081116535433071>, 384 : i64>, tensor<64x32xsi8, 1099511631744 : i64>, tensor<32xsi32, 1099511631232 : i64>, none, none) -> tensor<1x32x!quant.uniform<i8:f32, 0.022917204724409449>, 0 : i64> loc(#loc17)
      %17 = "tpu.MatMul"(%16, %9, %0, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1834483072], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [6]} : (tensor<1x32x!quant.uniform<i8:f32, 0.022917204724409449>, 0 : i64>, tensor<32x4xsi8, 1099511631104 : i64>, none, none, none) -> tensor<1x4x!quant.uniform<i8:f32, 0.025377328346456694>, 32 : i64> loc(#loc18)
      %18 = "tpu.MatMul"(%17, %10, %11, %0, %0) {do_relu = true, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1615084288], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [7]} : (tensor<1x4x!quant.uniform<i8:f32, 0.025377328346456694>, 32 : i64>, tensor<4x32xsi8, 1099511630976 : i64>, tensor<32xsi32, 1099511630848 : i64>, none, none) -> tensor<1x32x!quant.uniform<i8:f32, 0.019771251181102362>, 0 : i64> loc(#loc19)
      %19 = "tpu.MatMul"(%18, %12, %13, %0, %0) {do_relu = true, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1980202752], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [7]} : (tensor<1x32x!quant.uniform<i8:f32, 0.019771251181102362>, 0 : i64>, tensor<32x96xsi8, 1099511627776 : i64>, tensor<96xsi32, 1099511631360 : i64>, none, none) -> tensor<1x96x!quant.uniform<i8:f32, 0.012892165354330709>, 32 : i64> loc(#loc20)
      %20 = "tpu.MatMul"(%19, %14, %0, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1484088960], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [8]} : (tensor<1x96x!quant.uniform<i8:f32, 0.012892165354330709>, 32 : i64>, tensor<96x768xsi8, 1099511691712 : i64>, none, none, none) -> tensor<1x768x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> loc(#loc21)
      %21 = "tpu.Reshape"(%20) {flatten_start_dim = -1 : i64, shape = [1, 256, 3]} : (tensor<1x768x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64>) -> tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> loc(#loc22)
      return %21 : tensor<1x256x3x!quant.uniform<i8:f32, 0.008232239370078739>, 2199023255552 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/en_conv1/activate/Relu_output_0_Relu_bias_packed")
#loc3 = loc("/en_conv1/activate/Relu_output_0_Relu_filter_reordered")
#loc4 = loc("/en_conv1/activate/Relu_output_0_Relu")
#loc5 = loc("/en_conv2/activate/Relu_output_0_Relu_bias_packed")
#loc6 = loc("/en_conv2/activate/Relu_output_0_Relu_filter_reordered")
#loc7 = loc("/en_conv2/activate/Relu_output_0_Relu")
#loc8 = loc("/en_lin1/activate/Relu_output_0_Relu_filter_i8")
#loc9 = loc("/en_lin1/activate/Relu_output_0_Relu_bias_int32")
#loc10 = loc("/en_lin2/MatMul_output_0_MatMul_filter_i8")
#loc11 = loc("/de_lin1/activate/Relu_output_0_Relu_filter_i8")
#loc12 = loc("/de_lin1/activate/Relu_output_0_Relu_bias_int32")
#loc13 = loc("/de_lin2/activate/Relu_output_0_Relu_filter_i8")
#loc14 = loc("/de_lin2/activate/Relu_output_0_Relu_bias_int32")
#loc15 = loc("/out_lin/MatMul_output_0_MatMul_filter_i8")
#loc16 = loc("/Reshape_output_0_Reshape")
#loc17 = loc("/en_lin1/activate/Relu_output_0_Relu")
#loc18 = loc("/en_lin2/MatMul_output_0_MatMul")
#loc19 = loc("/de_lin1/activate/Relu_output_0_Relu")
#loc20 = loc("/de_lin2/activate/Relu_output_0_Relu")
#loc21 = loc("/out_lin/MatMul_output_0_MatMul")
#loc22 = loc("output_Reshape")

