#loc = loc(unknown)
#loc1 = loc("input")
module @unet attributes {module.FLOPs = 67808000 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv180x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["input"], module.mode = "INT8", module.outputs = ["/conv/Conv_output_0_Conv"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "unet_tpu_addressed_cv180x_int8_sym_weight.npz"} {
  module @unet attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 67072 : i64, module.device_id = 0 : i64, module.neuron_size = 130688 : i64, module.private_size = 0 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x80x80xf32> loc(unknown)) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", scale = [1.000000e+00, 1.000000e+00, 1.000000e+00]} : (tensor<1x3x80x80xf32>) -> tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>, 3298534883328 : i64> loc(#loc1)
      %1 = call @subfunc_0(%0) : (tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>, 3298534883328 : i64>) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> loc(#loc)
      return %1 : tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>, 3298534883328 : i64> loc("input")) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099511631344 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x4x9x4xsi8, 1099511634352 : i64> loc(#loc3)
      %3 = "top.Weight"() : () -> tensor<1x8x1x9xi8, 1099511634512 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x8x9x4xsi8, 1099511674672 : i64> loc(#loc5)
      %5:3 = "tpu.Group"(%arg0) ({
        %45 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 4 : i64} : (tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>, 3298534883328 : i64>) -> tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>> loc(#loc9)
        %46 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14400, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x9x4xsi8, 1099511634352 : i64>) -> tensor<1x4x9x4xsi8> loc(#loc10)
        %47 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14480, out_size = 18, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099511631344 : i64>) -> tensor<1x4x1x9xi8> loc(#loc11)
        %48 = "tpu.Conv2D"(%45, %46, %47) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 3, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>>, tensor<1x4x9x4xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>> loc(#loc6)
        %49 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16000, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x9x4xsi8, 1099511674672 : i64>) -> tensor<1x8x9x4xsi8> loc(#loc12)
        %50 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16144, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x1x9xi8, 1099511634512 : i64>) -> tensor<1x8x1x9xi8> loc(#loc13)
        %51 = "tpu.Store"(%48, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>>, none) -> tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 76800 : i64> loc(#loc6)
        %52 = "tpu.Pool2D"(%48) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 12800, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>>) -> tensor<1x4x40x40x!quant.uniform<i8:f32, 0.035380446456692916>> loc(#loc14)
        %53 = "tpu.Conv2D"(%52, %49, %50) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x4x40x40x!quant.uniform<i8:f32, 0.035380446456692916>>, tensor<1x8x9x4xsi8>, tensor<1x8x1x9xi8>) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>> loc(#loc7)
        %54 = "tpu.Store"(%53, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>>, none) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 117888 : i64> loc(#loc7)
        %55 = "tpu.Pool2D"(%53) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 12800, out_size = 1600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>>) -> tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>> loc(#loc8)
        %56 = "tpu.Store"(%55, %0) {ginfo = #tpu.lg<out_addr = 12800, out_size = 1600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>>, none) -> tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64> loc(#loc8)
        "tpu.Yield"(%51, %54, %56) : (tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 76800 : i64>, tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 117888 : i64>, tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64>) -> () loc(#loc72)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 11, -2, 7, 4, 5, 6, -3, 8, 0, -4, 10, 9, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x80x80x!quant.uniform<i8:f32, 0.033482382677165357>, 3298534883328 : i64>) -> (tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 76800 : i64>, tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 117888 : i64>, tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64>) loc(#loc72)
      %6 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511672032 : i64> loc(#loc15)
      %7 = "top.Weight"() : () -> tensor<1x32x9x8xsi8, 1099511672368 : i64> loc(#loc16)
      %8 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511634592 : i64> loc(#loc17)
      %9 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099511635168 : i64> loc(#loc18)
      %10:2 = "tpu.Group"(%5#2) ({
        %45 = "tpu.Load"(%5#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64>) -> tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>> loc(#loc21)
        %46 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6400, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [8], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x8xsi8, 1099511672368 : i64>) -> tensor<1x32x9x8xsi8> loc(#loc22)
        %47 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7840, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511672032 : i64>) -> tensor<1x32x1x9xi8> loc(#loc23)
        %48 = "tpu.Conv2D"(%45, %46, %47) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>>, tensor<1x32x9x8xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>> loc(#loc19)
        %49 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099511635168 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc24)
        %50 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7552, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511634592 : i64>) -> tensor<1x64x1x9xi8> loc(#loc25)
        %51 = "tpu.Pool2D"(%48) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 24576, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [10], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>>) -> tensor<1x32x10x10x!quant.uniform<i8:f32, 0.052535254330708658>> loc(#loc26)
        %52 = "tpu.Store"(%48, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>>, none) -> tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 38400 : i64> loc(#loc19)
        %53 = "tpu.Conv2D"(%51, %49, %50) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [10], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x10x10x!quant.uniform<i8:f32, 0.052535254330708658>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x10x10x!quant.uniform<i8:f32, 0.046114373228346453>> loc(#loc20)
        %54 = "tpu.Store"(%53, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [10], w_idx = [0], w_slice = [10], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x10x10x!quant.uniform<i8:f32, 0.046114373228346453>>, none) -> tensor<1x64x10x10x!quant.uniform<i8:f32, 0.046114373228346453>, 0 : i64> loc(#loc20)
        "tpu.Yield"(%52, %54) : (tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 38400 : i64>, tensor<1x64x10x10x!quant.uniform<i8:f32, 0.046114373228346453>, 0 : i64>) -> () loc(#loc73)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 9, -2, 6, 4, 5, -3, 8, 0, 1, 2, 7], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x8x20x20x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64>) -> (tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 38400 : i64>, tensor<1x64x10x10x!quant.uniform<i8:f32, 0.046114373228346453>, 0 : i64>) loc(#loc73)
      %11 = "tpu.Pad"(%10#1, %0, %0, %0, %0) {mode = #tpu<Pad_Mode edge>, paddings = [0, 0, 1, 1, 0, 0, 1, 1], val = 0.000000e+00 : f64} : (tensor<1x64x10x10x!quant.uniform<i8:f32, 0.046114373228346453>, 0 : i64>, none, none, none, none) -> tensor<1x64x12x12x!quant.uniform<i8:f32, 0.046114373228346453>, 102400 : i64> loc(#loc27)
      %12 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x5xi8, 1099511631392 : i64> loc(#loc28)
      %13 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x4x4xsi8, 1099511674960 : i64> loc(#loc29)
      %14 = "tpu.Conv2D"(%11, %13, %12) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 64 : i64, inserts = [1, 1], kernel_shape = [4, 4], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x64x12x12x!quant.uniform<i8:f32, 0.046114373228346453>, 102400 : i64>, tensor<1x64x4x4xsi8, 1099511674960 : i64>, tensor<1x64x1x5xi8, 1099511631392 : i64>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.043964918897637796>, 0 : i64> loc(#loc30)
      %15 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x1x9xi8, 1099511631712 : i64> loc(#loc31)
      %16 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x9x64xsi8, 1099511676128 : i64> loc(#loc32)
      %17 = "tpu.Conv2D"(%14, %16, %15) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.043964918897637796>, 0 : i64>, tensor<1x32x9x64xsi8, 1099511676128 : i64>, tensor<1x32x1x9xi8, 1099511631712 : i64>) -> tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 25600 : i64> loc(#loc33)
      %18 = "tpu.Concat"(%17, %10#0) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 25600 : i64>, tensor<1x32x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 38400 : i64>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 25600 : i64> loc(#loc34)
      %19 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x1x9xi8, 1099511694560 : i64> loc(#loc35)
      %20 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x9x64xsi8, 1099511653600 : i64> loc(#loc36)
      %21 = "tpu.Conv2D"(%18, %20, %19) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.052535254330708658>, 25600 : i64>, tensor<1x32x9x64xsi8, 1099511653600 : i64>, tensor<1x32x1x9xi8, 1099511694560 : i64>) -> tensor<1x32x20x20x!quant.uniform<i8:f32, 0.036201825984251965>, 0 : i64> loc(#loc37)
      %22 = "tpu.Pad"(%21, %0, %0, %0, %0) {mode = #tpu<Pad_Mode edge>, paddings = [0, 0, 1, 1, 0, 0, 1, 1], val = 0.000000e+00 : f64} : (tensor<1x32x20x20x!quant.uniform<i8:f32, 0.036201825984251965>, 0 : i64>, none, none, none, none) -> tensor<1x32x22x22x!quant.uniform<i8:f32, 0.036201825984251965>, 102400 : i64> loc(#loc38)
      %23 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x1x5xi8, 1099511631056 : i64> loc(#loc39)
      %24 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x4x4xsi8, 1099511630544 : i64> loc(#loc40)
      %25 = "tpu.Conv2D"(%22, %24, %23) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 32 : i64, inserts = [1, 1], kernel_shape = [4, 4], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x32x22x22x!quant.uniform<i8:f32, 0.036201825984251965>, 102400 : i64>, tensor<1x32x4x4xsi8, 1099511630544 : i64>, tensor<1x32x1x5xi8, 1099511631056 : i64>) -> tensor<1x32x40x40x!quant.uniform<i8:f32, 0.033215905511811021>, 0 : i64> loc(#loc41)
      %26 = "top.Weight"() {do_compress = true} : () -> tensor<1x8x1x9xi8, 1099511630464 : i64> loc(#loc42)
      %27 = "top.Weight"() {do_compress = true} : () -> tensor<1x8x9x32xsi8, 1099511632048 : i64> loc(#loc43)
      %28 = "tpu.Conv2D"(%25, %27, %26) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x40x40x!quant.uniform<i8:f32, 0.033215905511811021>, 0 : i64>, tensor<1x8x9x32xsi8, 1099511632048 : i64>, tensor<1x8x1x9xi8, 1099511630464 : i64>) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64> loc(#loc44)
      %29 = "top.Weight"() : () -> tensor<1x8x1x9xi8, 1099511630384 : i64> loc(#loc45)
      %30 = "top.Weight"() : () -> tensor<1x8x9x16xsi8, 1099511629232 : i64> loc(#loc46)
      %31 = "tpu.Group"(%28, %5#1) ({
        %45 = "tpu.Load"(%28) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3392, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0, 19], h_slice = [21, 21], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64>) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>> loc(#loc48)
        %46 = "tpu.Load"(%5#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3392, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0, 19], h_slice = [21, 21], w_idx = [0], w_slice = [40], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 117888 : i64>) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>> loc(#loc49)
        %47 = "tpu.Load"(%30) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x9x16xsi8, 1099511629232 : i64>) -> tensor<1x8x9x16xsi8> loc(#loc50)
        %48 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x1x9xi8, 1099511630384 : i64>) -> tensor<1x8x1x9xi8> loc(#loc51)
        %49 = "tpu.Concat"(%45, %46) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6784, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 19], h_slice = [21, 21], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>>, tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>>) -> tensor<1x16x40x40x!quant.uniform<i8:f32, 0.037113569291338577>> loc(#loc52)
        %50 = "tpu.Conv2D"(%49, %47, %48) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x40x40x!quant.uniform<i8:f32, 0.037113569291338577>>, tensor<1x8x9x16xsi8>, tensor<1x8x1x9xi8>) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037348796062992129>> loc(#loc47)
        %51 = "tpu.Store"(%50, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037348796062992129>>, none) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037348796062992129>, 0 : i64> loc(#loc47)
        "tpu.Yield"(%51) : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037348796062992129>, 0 : i64>) -> () loc(#loc47)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 102400 : i64>, tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037113569291338577>, 117888 : i64>) -> tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037348796062992129>, 0 : i64> loc(#loc47)
      %32 = "tpu.Pad"(%31, %0, %0, %0, %0) {mode = #tpu<Pad_Mode edge>, paddings = [0, 0, 1, 1, 0, 0, 1, 1], val = 0.000000e+00 : f64} : (tensor<1x8x40x40x!quant.uniform<i8:f32, 0.037348796062992129>, 0 : i64>, none, none, none, none) -> tensor<1x8x42x42x!quant.uniform<i8:f32, 0.037348796062992129>, 102400 : i64> loc(#loc53)
      %33 = "top.Weight"() {do_compress = true} : () -> tensor<1x8x1x5xi8, 1099511672320 : i64> loc(#loc54)
      %34 = "top.Weight"() {do_compress = true} : () -> tensor<1x8x4x4xsi8, 1099511631216 : i64> loc(#loc55)
      %35 = "tpu.Conv2D"(%32, %34, %33) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 8 : i64, inserts = [1, 1], kernel_shape = [4, 4], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x8x42x42x!quant.uniform<i8:f32, 0.037348796062992129>, 102400 : i64>, tensor<1x8x4x4xsi8, 1099511631216 : i64>, tensor<1x8x1x5xi8, 1099511672320 : i64>) -> tensor<1x8x80x80x!quant.uniform<i8:f32, 0.031167262992125983>, 0 : i64> loc(#loc56)
      %36 = "top.Weight"() {do_compress = true} : () -> tensor<1x4x1x9xi8, 1099511632000 : i64> loc(#loc57)
      %37 = "top.Weight"() {do_compress = true} : () -> tensor<1x4x9x8xsi8, 1099511628944 : i64> loc(#loc58)
      %38 = "tpu.Conv2D"(%35, %37, %36) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x8x80x80x!quant.uniform<i8:f32, 0.031167262992125983>, 0 : i64>, tensor<1x4x9x8xsi8, 1099511628944 : i64>, tensor<1x4x1x9xi8, 1099511632000 : i64>) -> tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 51200 : i64> loc(#loc59)
      %39 = "tpu.Concat"(%38, %5#0) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 51200 : i64>, tensor<1x4x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 76800 : i64>) -> tensor<1x8x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 51200 : i64> loc(#loc60)
      %40 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511675984 : i64> loc(#loc61)
      %41 = "top.Weight"() : () -> tensor<1x16x9x8xsi8, 1099511627792 : i64> loc(#loc62)
      %42 = "top.Weight"() : () -> tensor<1x1x1x9xi8, 1099511627776 : i64> loc(#loc63)
      %43 = "top.Weight"() : () -> tensor<1x1x1x16xsi8, 1099511634496 : i64> loc(#loc64)
      %44 = "tpu.Group"(%39) ({
        %45 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 9280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [8], d_idx = [0], d_slice = [1], h_idx = [0, 26, 53], h_slice = [28, 29, 27], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x8x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 51200 : i64>) -> tensor<1x8x80x80x!quant.uniform<i8:f32, 0.035380446456692916>> loc(#loc66)
        %46 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19440, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [8], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x8xsi8, 1099511627792 : i64>) -> tensor<1x16x9x8xsi8> loc(#loc67)
        %47 = "tpu.Load"(%40) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20016, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511675984 : i64>) -> tensor<1x16x1x9xi8> loc(#loc68)
        %48 = "tpu.Load"(%43) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20096, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [16], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x16xsi8, 1099511634496 : i64>) -> tensor<1x1x1x16xsi8> loc(#loc69)
        %49 = "tpu.Load"(%42) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20112, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x9xi8, 1099511627776 : i64>) -> tensor<1x1x1x9xi8> loc(#loc70)
        %50 = "tpu.Conv2D"(%45, %46, %47) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 17280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 27, 54], h_slice = [27, 27, 26], w_idx = [0], w_slice = [80], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x8x80x80x!quant.uniform<i8:f32, 0.035380446456692916>>, tensor<1x16x9x8xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x80x80x!quant.uniform<i8:f32, 0.030290658267716538>> loc(#loc71)
        %51 = "tpu.Conv2D"(%50, %48, %49) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 17280, out_size = 2160, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0, 27, 54], h_slice = [27, 27, 26], w_idx = [0], w_slice = [80], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x80x80x!quant.uniform<i8:f32, 0.030290658267716538>>, tensor<1x1x1x16xsi8>, tensor<1x1x1x9xi8>) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>> loc(#loc65)
        %52 = "tpu.Store"(%51, %0) {ginfo = #tpu.lg<out_addr = 17280, out_size = 2160, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0, 27, 54], h_slice = [27, 27, 26], w_idx = [0], w_slice = [80], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>>, none) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> loc(#loc65)
        "tpu.Yield"(%52) : (tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64>) -> () loc(#loc65)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 7, -2, 6, 0, 1, 2], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x8x80x80x!quant.uniform<i8:f32, 0.035380446456692916>, 51200 : i64>) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> loc(#loc65)
      return %44 : tensor<1x1x80x80x!quant.uniform<i8:f32, 0.016697232283464567>, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/enc1/activate/Relu_output_0_Relu_bias_packed")
#loc3 = loc("/enc1/activate/Relu_output_0_Relu_filter_reordered")
#loc4 = loc("/enc2/activate/Relu_output_0_Relu_bias_packed")
#loc5 = loc("/enc2/activate/Relu_output_0_Relu_filter_reordered")
#loc6 = loc("/enc1/activate/Relu_output_0_Relu")
#loc7 = loc("/enc2/activate/Relu_output_0_Relu")
#loc8 = loc("/enc3/pool/MaxPool_output_0_MaxPool")
#loc9 = loc("load_0")
#loc10 = loc("load_/enc1/activate/Relu_output_0_Relu_filter_reordered")
#loc11 = loc("load_/enc1/activate/Relu_output_0_Relu_bias_packed")
#loc12 = loc("load_/enc2/activate/Relu_output_0_Relu_filter_reordered")
#loc13 = loc("load_/enc2/activate/Relu_output_0_Relu_bias_packed")
#loc14 = loc("/enc2/pool/MaxPool_output_0_MaxPool")
#loc15 = loc("/enc3/activate/Relu_output_0_Relu_bias_packed")
#loc16 = loc("/enc3/activate/Relu_output_0_Relu_filter_reordered")
#loc17 = loc("/bneck/activate/Relu_output_0_Relu_bias_packed")
#loc18 = loc("/bneck/activate/Relu_output_0_Relu_filter_reordered")
#loc19 = loc("/enc3/activate/Relu_output_0_Relu")
#loc20 = loc("/bneck/activate/Relu_output_0_Relu")
#loc21 = loc("load_/enc3/pool/MaxPool_output_0_MaxPool")
#loc22 = loc("load_/enc3/activate/Relu_output_0_Relu_filter_reordered")
#loc23 = loc("load_/enc3/activate/Relu_output_0_Relu_bias_packed")
#loc24 = loc("load_/bneck/activate/Relu_output_0_Relu_filter_reordered")
#loc25 = loc("load_/bneck/activate/Relu_output_0_Relu_bias_packed")
#loc26 = loc("/bneck/pool/MaxPool_output_0_MaxPool")
#loc27 = loc("/bneck/activate/Relu_output_0_Relu_pad_edge")
#loc28 = loc("/Resize_output_0_Resize_bias_packed")
#loc29 = loc("/Resize_output_0_Resize_filter_reordered")
#loc30 = loc("/Resize_output_0_Resize")
#loc31 = loc("/upconv3/activate/Relu_output_0_Relu_bias_packed")
#loc32 = loc("/upconv3/activate/Relu_output_0_Relu_filter_reordered")
#loc33 = loc("/upconv3/activate/Relu_output_0_Relu")
#loc34 = loc("/Concat_output_0_Concat")
#loc35 = loc("/dec3/activate/Relu_output_0_Relu_bias_packed")
#loc36 = loc("/dec3/activate/Relu_output_0_Relu_filter_reordered")
#loc37 = loc("/dec3/activate/Relu_output_0_Relu")
#loc38 = loc("/dec3/activate/Relu_output_0_Relu_pad_edge")
#loc39 = loc("/Resize_1_output_0_Resize_bias_packed")
#loc40 = loc("/Resize_1_output_0_Resize_filter_reordered")
#loc41 = loc("/Resize_1_output_0_Resize")
#loc42 = loc("/upconv2/activate/Relu_output_0_Relu_bias_packed")
#loc43 = loc("/upconv2/activate/Relu_output_0_Relu_filter_reordered")
#loc44 = loc("/upconv2/activate/Relu_output_0_Relu")
#loc45 = loc("/dec2/activate/Relu_output_0_Relu_bias_packed")
#loc46 = loc("/dec2/activate/Relu_output_0_Relu_filter_reordered")
#loc47 = loc("/dec2/activate/Relu_output_0_Relu")
#loc48 = loc("load_/upconv2/activate/Relu_output_0_Relu")
#loc49 = loc("load_/enc2/activate/Relu_output_0_Relu")
#loc50 = loc("load_/dec2/activate/Relu_output_0_Relu_filter_reordered")
#loc51 = loc("load_/dec2/activate/Relu_output_0_Relu_bias_packed")
#loc52 = loc("/Concat_1_output_0_Concat")
#loc53 = loc("/dec2/activate/Relu_output_0_Relu_pad_edge")
#loc54 = loc("/Resize_2_output_0_Resize_bias_packed")
#loc55 = loc("/Resize_2_output_0_Resize_filter_reordered")
#loc56 = loc("/Resize_2_output_0_Resize")
#loc57 = loc("/upconv1/activate/Relu_output_0_Relu_bias_packed")
#loc58 = loc("/upconv1/activate/Relu_output_0_Relu_filter_reordered")
#loc59 = loc("/upconv1/activate/Relu_output_0_Relu")
#loc60 = loc("/Concat_2_output_0_Concat")
#loc61 = loc("/dec1/activate/Relu_output_0_Relu_bias_packed")
#loc62 = loc("/dec1/activate/Relu_output_0_Relu_filter_reordered")
#loc63 = loc("/conv/Conv_output_0_Conv_bias_packed")
#loc64 = loc("/conv/Conv_output_0_Conv_filter_reordered")
#loc65 = loc("/conv/Conv_output_0_Conv")
#loc66 = loc("load_/Concat_2_output_0_Concat")
#loc67 = loc("load_/dec1/activate/Relu_output_0_Relu_filter_reordered")
#loc68 = loc("load_/dec1/activate/Relu_output_0_Relu_bias_packed")
#loc69 = loc("load_/conv/Conv_output_0_Conv_filter_reordered")
#loc70 = loc("load_/conv/Conv_output_0_Conv_bias_packed")
#loc71 = loc("/dec1/activate/Relu_output_0_Relu")
#loc72 = loc(fused[#loc6, #loc7, #loc8])
#loc73 = loc(fused[#loc19, #loc20])

