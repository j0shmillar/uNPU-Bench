#loc = loc(unknown)
#loc1 = loc("input")
module @nas attributes {module.FLOPs = 73269258 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv180x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["input"], module.mode = "INT8", module.outputs = ["/fc/Gemm_output_0_Gemm"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "nas_tpu_addressed_cv180x_int8_sym_weight.npz"} {
  module @nas attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 362480 : i64, module.device_id = 0 : i64, module.neuron_size = 20480 : i64, module.private_size = 0 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x32x32xf32> loc(unknown)) -> tensor<1x10x!quant.uniform<i8:f32, 0.036896622834645669>, 4398046511104 : i64> {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", scale = [1.000000e+00, 1.000000e+00, 1.000000e+00]} : (tensor<1x3x32x32xf32>) -> tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>, 3298534883328 : i64> loc(#loc1)
      %1 = call @subfunc_0(%0) : (tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>, 3298534883328 : i64>) -> tensor<1x10x!quant.uniform<i8:f32, 0.036896622834645669>, 4398046511104 : i64> loc(#loc)
      return %1 : tensor<1x10x!quant.uniform<i8:f32, 0.036896622834645669>, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>, 3298534883328 : i64> loc("input")) -> tensor<1x10x!quant.uniform<i8:f32, 0.036896622834645669>, 4398046511104 : i64> attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511712960 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x64x9x4xsi8, 1099511713536 : i64> loc(#loc3)
      %3 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511715888 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x32x1x64xsi8, 1099511732560 : i64> loc(#loc5)
      %5 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511734608 : i64> loc(#loc6)
      %6 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099511970672 : i64> loc(#loc7)
      %7 = "tpu.Group"(%arg0) ({
        %34 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 6, 14, 22], h_slice = [10, 12, 12, 10], w_idx = [0], w_slice = [32], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 4 : i64} : (tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>, 3298534883328 : i64>) -> tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>> loc(#loc9)
        %35 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x4xsi8, 1099511713536 : i64>) -> tensor<1x64x9x4xsi8> loc(#loc10)
        %36 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11824, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511712960 : i64>) -> tensor<1x64x1x9xi8> loc(#loc11)
        %37 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10368, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x64xsi8, 1099511732560 : i64>) -> tensor<1x32x1x64xsi8> loc(#loc12)
        %38 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11680, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511715888 : i64>) -> tensor<1x32x1x9xi8> loc(#loc13)
        %39 = "tpu.Conv2D"(%34, %35, %36) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7, 15, 23], h_slice = [9, 10, 10, 9], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>>, tensor<1x64x9x4xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x32x32x!quant.uniform<i8:f32, 0.095447412598425194>> loc(#loc14)
        %40 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099511970672 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc15)
        %41 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11392, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511734608 : i64>) -> tensor<1x64x1x9xi8> loc(#loc16)
        %42 = "tpu.Conv2D"(%39, %37, %38) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 7, 15, 23], h_slice = [9, 10, 10, 9], w_idx = [0], w_slice = [32], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x32x32x!quant.uniform<i8:f32, 0.095447412598425194>>, tensor<1x32x1x64xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x32x32x!quant.uniform<i8:f32, 0.07603916377952756>> loc(#loc17)
        %43 = "tpu.Conv2D"(%42, %40, %41) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24], h_slice = [8, 8, 8, 8], w_idx = [0], w_slice = [32], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x32x32x!quant.uniform<i8:f32, 0.07603916377952756>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x32x32x!quant.uniform<i8:f32, 0.063911250393700797>> loc(#loc18)
        %44 = "tpu.Pool2D"(%43) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 24576, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12], h_slice = [4, 4, 4, 4], w_idx = [0], w_slice = [16], id = 10, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x64x32x32x!quant.uniform<i8:f32, 0.063911250393700797>>) -> tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>> loc(#loc8)
        %45 = "tpu.Store"(%44, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12], h_slice = [4, 4, 4, 4], w_idx = [0], w_slice = [16], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>>, none) -> tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>, 0 : i64> loc(#loc8)
        "tpu.Yield"(%45) : (tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>, 0 : i64>) -> () loc(#loc8)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 11, -2, 8, 6, 7, -3, 9, -4, 10, 0, 1, 2], group_type = 0 : i64, hsecs = 4 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-4, 0, 2], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x32x32x!quant.uniform<i8:f32, 0.02612554960629921>, 3298534883328 : i64>) -> tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>, 0 : i64> loc(#loc8)
      %8 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511803344 : i64> loc(#loc19)
      %9 = "top.Weight"() : () -> tensor<1x32x9x64xsi8, 1099511803632 : i64> loc(#loc20)
      %10 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511802768 : i64> loc(#loc21)
      %11 = "top.Weight"() : () -> tensor<1x64x1x32xsi8, 1099511735184 : i64> loc(#loc22)
      %12 = "tpu.Group"(%7) ({
        %34 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x64xsi8, 1099511803632 : i64>) -> tensor<1x32x9x64xsi8> loc(#loc24)
        %35 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [9, 9], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>, 0 : i64>) -> tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>> loc(#loc25)
        %36 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9504, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511803344 : i64>) -> tensor<1x32x1x9xi8> loc(#loc26)
        %37 = "tpu.Load"(%11) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x32xsi8, 1099511735184 : i64>) -> tensor<1x64x1x32xsi8> loc(#loc27)
        %38 = "tpu.Load"(%10) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511802768 : i64>) -> tensor<1x64x1x9xi8> loc(#loc28)
        %39 = "tpu.Conv2D"(%35, %34, %36) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8], h_slice = [8, 8], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>>, tensor<1x32x9x64xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x16x16x!quant.uniform<i8:f32, 0.041952493700787401>> loc(#loc29)
        %40 = "tpu.Conv2D"(%39, %37, %38) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8], h_slice = [8, 8], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x16x16x!quant.uniform<i8:f32, 0.041952493700787401>>, tensor<1x64x1x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x16x16x!quant.uniform<i8:f32, 0.040173811811023627>> loc(#loc30)
        %41 = "tpu.Pool2D"(%40) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 29696, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 4], w_idx = [0], w_slice = [8], id = 7, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x64x16x16x!quant.uniform<i8:f32, 0.040173811811023627>>) -> tensor<1x64x8x8x!quant.uniform<i8:f32, 0.040173811811023627>> loc(#loc23)
        %42 = "tpu.Store"(%41, %0) {ginfo = #tpu.lg<out_addr = 29696, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 4], w_idx = [0], w_slice = [8], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x8x8x!quant.uniform<i8:f32, 0.040173811811023627>>, none) -> tensor<1x64x8x8x!quant.uniform<i8:f32, 0.040173811811023627>, 16384 : i64> loc(#loc23)
        "tpu.Yield"(%42) : (tensor<1x64x8x8x!quant.uniform<i8:f32, 0.040173811811023627>, 16384 : i64>) -> () loc(#loc23)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 8, -2, 6, 0, -3, 7, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [2, 0], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x16x16x!quant.uniform<i8:f32, 0.063911250393700797>, 0 : i64>) -> tensor<1x64x8x8x!quant.uniform<i8:f32, 0.040173811811023627>, 16384 : i64> loc(#loc23)
      %13 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099511822064 : i64> loc(#loc31)
      %14 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x64xsi8, 1099511896944 : i64> loc(#loc32)
      %15 = "tpu.Conv2D"(%12, %14, %13) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x8x8x!quant.uniform<i8:f32, 0.040173811811023627>, 16384 : i64>, tensor<1x128x9x64xsi8, 1099511896944 : i64>, tensor<1x128x1x9xi8, 1099511822064 : i64>) -> tensor<1x128x8x8x!quant.uniform<i8:f32, 0.028750859055118112>, 0 : i64> loc(#loc33)
      %16 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099511989104 : i64> loc(#loc34)
      %17 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099511716176 : i64> loc(#loc35)
      %18 = "tpu.Group"(%15) ({
        %34 = "tpu.Load"(%15) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x8x8x!quant.uniform<i8:f32, 0.028750859055118112>, 0 : i64>) -> tensor<1x128x8x8x!quant.uniform<i8:f32, 0.028750859055118112>> loc(#loc37)
        %35 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099511716176 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc38)
        %36 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099511989104 : i64>) -> tensor<1x128x1x9xi8> loc(#loc39)
        %37 = "tpu.Conv2D"(%34, %35, %36) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 3, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x8x8x!quant.uniform<i8:f32, 0.028750859055118112>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x8x8x!quant.uniform<i8:f32, 0.027741612598425197>> loc(#loc40)
        %38 = "tpu.Pool2D"(%37) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 16384, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [4], w_idx = [0], w_slice = [4], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x128x8x8x!quant.uniform<i8:f32, 0.027741612598425197>>) -> tensor<1x128x4x4x!quant.uniform<i8:f32, 0.027741612598425197>> loc(#loc36)
        %39 = "tpu.Store"(%38, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [4], w_idx = [0], w_slice = [4], id = 5, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x4x4x!quant.uniform<i8:f32, 0.027741612598425197>>, none) -> tensor<1x128x4x4x!quant.uniform<i8:f32, 0.027741612598425197>, 8192 : i64> loc(#loc36)
        "tpu.Yield"(%39) : (tensor<1x128x4x4x!quant.uniform<i8:f32, 0.027741612598425197>, 8192 : i64>) -> () loc(#loc36)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 5, -2, 4, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x8x8x!quant.uniform<i8:f32, 0.028750859055118112>, 0 : i64>) -> tensor<1x128x4x4x!quant.uniform<i8:f32, 0.027741612598425197>, 8192 : i64> loc(#loc36)
      %19 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099511712384 : i64> loc(#loc41)
      %20 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x128xsi8, 1099511638656 : i64> loc(#loc42)
      %21 = "tpu.Conv2D"(%18, %20, %19) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x4x4x!quant.uniform<i8:f32, 0.027741612598425197>, 8192 : i64>, tensor<1x64x9x128xsi8, 1099511638656 : i64>, tensor<1x64x1x9xi8, 1099511712384 : i64>) -> tensor<1x64x4x4x!quant.uniform<i8:f32, 0.021348011023622049>, 0 : i64> loc(#loc43)
      %22 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099511637504 : i64> loc(#loc44)
      %23 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x64xsi8, 1099511823216 : i64> loc(#loc45)
      %24 = "tpu.Conv2D"(%21, %23, %22) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x4x4x!quant.uniform<i8:f32, 0.021348011023622049>, 0 : i64>, tensor<1x128x9x64xsi8, 1099511823216 : i64>, tensor<1x128x1x9xi8, 1099511637504 : i64>) -> tensor<1x128x4x4x!quant.uniform<i8:f32, 0.023958012598425199>, 1024 : i64> loc(#loc46)
      %25 = "tpu.Pool2D"(%24) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x128x4x4x!quant.uniform<i8:f32, 0.023958012598425199>, 1024 : i64>) -> tensor<1x128x2x2x!quant.uniform<i8:f32, 0.023958012598425199>, 0 : i64> loc(#loc47)
      %26 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099511632896 : i64> loc(#loc48)
      %27 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x128xsi8, 1099511737232 : i64> loc(#loc49)
      %28 = "tpu.Conv2D"(%25, %27, %26) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x2x2x!quant.uniform<i8:f32, 0.023958012598425199>, 0 : i64>, tensor<1x512x1x128xsi8, 1099511737232 : i64>, tensor<1x512x1x9xi8, 1099511632896 : i64>) -> tensor<1x512x2x2x!quant.uniform<i8:f32, 0.020619854330708662>, 512 : i64> loc(#loc50)
      %29 = "tpu.Pool2D"(%28) {count_include_pad = true, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], multiplier = 106 : si32, pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Avg>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshift = 8 : si32, strides = [1, 1]} : (tensor<1x512x2x2x!quant.uniform<i8:f32, 0.020619854330708662>, 512 : i64>) -> tensor<1x512x1x1x!quant.uniform<i8:f32, 0.012391530708661417>, 0 : i64> loc(#loc51)
      %30 = "top.Weight"() {do_compress = true} : () -> tensor<512x10xsi8, 1099511627776 : i64> loc(#loc52)
      %31 = "top.Weight"() {do_compress = true} : () -> tensor<10xsi32, 1099511715840 : i64> loc(#loc53)
      %32 = "tpu.Reshape"(%29) {flatten_start_dim = 1 : i64, shape = [1, 512]} : (tensor<1x512x1x1x!quant.uniform<i8:f32, 0.012391530708661417>, 0 : i64>) -> tensor<1x512x!quant.uniform<i8:f32, 0.012391530708661417>, 0 : i64> loc(#loc54)
      %33 = "tpu.MatMul"(%32, %30, %31, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1712543616], output_transpose = false, quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [10]} : (tensor<1x512x!quant.uniform<i8:f32, 0.012391530708661417>, 0 : i64>, tensor<512x10xsi8, 1099511627776 : i64>, tensor<10xsi32, 1099511715840 : i64>, none, none) -> tensor<1x10x!quant.uniform<i8:f32, 0.036896622834645669>, 4398046511104 : i64> loc(#loc55)
      return %33 : tensor<1x10x!quant.uniform<i8:f32, 0.036896622834645669>, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/conv1_1/activate/Relu_output_0_Relu_bias_packed")
#loc3 = loc("/conv1_1/activate/Relu_output_0_Relu_filter_reordered")
#loc4 = loc("/conv1_2/activate/Relu_output_0_Relu_bias_packed")
#loc5 = loc("/conv1_2/activate/Relu_output_0_Relu_filter_reordered")
#loc6 = loc("/conv1_3/activate/Relu_output_0_Relu_bias_packed")
#loc7 = loc("/conv1_3/activate/Relu_output_0_Relu_filter_reordered")
#loc8 = loc("/conv2_1/pool/MaxPool_output_0_MaxPool")
#loc9 = loc("load_0")
#loc10 = loc("load_/conv1_1/activate/Relu_output_0_Relu_filter_reordered")
#loc11 = loc("load_/conv1_1/activate/Relu_output_0_Relu_bias_packed")
#loc12 = loc("load_/conv1_2/activate/Relu_output_0_Relu_filter_reordered")
#loc13 = loc("load_/conv1_2/activate/Relu_output_0_Relu_bias_packed")
#loc14 = loc("/conv1_1/activate/Relu_output_0_Relu")
#loc15 = loc("load_/conv1_3/activate/Relu_output_0_Relu_filter_reordered")
#loc16 = loc("load_/conv1_3/activate/Relu_output_0_Relu_bias_packed")
#loc17 = loc("/conv1_2/activate/Relu_output_0_Relu")
#loc18 = loc("/conv1_3/activate/Relu_output_0_Relu")
#loc19 = loc("/conv2_1/activate/Relu_output_0_Relu_bias_packed")
#loc20 = loc("/conv2_1/activate/Relu_output_0_Relu_filter_reordered")
#loc21 = loc("/conv2_2/activate/Relu_output_0_Relu_bias_packed")
#loc22 = loc("/conv2_2/activate/Relu_output_0_Relu_filter_reordered")
#loc23 = loc("/conv3_1/pool/MaxPool_output_0_MaxPool")
#loc24 = loc("load_/conv2_1/activate/Relu_output_0_Relu_filter_reordered")
#loc25 = loc("load_/conv2_1/pool/MaxPool_output_0_MaxPool")
#loc26 = loc("load_/conv2_1/activate/Relu_output_0_Relu_bias_packed")
#loc27 = loc("load_/conv2_2/activate/Relu_output_0_Relu_filter_reordered")
#loc28 = loc("load_/conv2_2/activate/Relu_output_0_Relu_bias_packed")
#loc29 = loc("/conv2_1/activate/Relu_output_0_Relu")
#loc30 = loc("/conv2_2/activate/Relu_output_0_Relu")
#loc31 = loc("/conv3_1/activate/Relu_output_0_Relu_bias_packed")
#loc32 = loc("/conv3_1/activate/Relu_output_0_Relu_filter_reordered")
#loc33 = loc("/conv3_1/activate/Relu_output_0_Relu")
#loc34 = loc("/conv3_2/activate/Relu_output_0_Relu_bias_packed")
#loc35 = loc("/conv3_2/activate/Relu_output_0_Relu_filter_reordered")
#loc36 = loc("/conv4_1/pool/MaxPool_output_0_MaxPool")
#loc37 = loc("load_/conv3_1/activate/Relu_output_0_Relu")
#loc38 = loc("load_/conv3_2/activate/Relu_output_0_Relu_filter_reordered")
#loc39 = loc("load_/conv3_2/activate/Relu_output_0_Relu_bias_packed")
#loc40 = loc("/conv3_2/activate/Relu_output_0_Relu")
#loc41 = loc("/conv4_1/activate/Relu_output_0_Relu_bias_packed")
#loc42 = loc("/conv4_1/activate/Relu_output_0_Relu_filter_reordered")
#loc43 = loc("/conv4_1/activate/Relu_output_0_Relu")
#loc44 = loc("/conv4_2/activate/Relu_output_0_Relu_bias_packed")
#loc45 = loc("/conv4_2/activate/Relu_output_0_Relu_filter_reordered")
#loc46 = loc("/conv4_2/activate/Relu_output_0_Relu")
#loc47 = loc("/conv5_1/pool/MaxPool_output_0_MaxPool")
#loc48 = loc("/conv5_1/activate/Relu_output_0_Relu_bias_packed")
#loc49 = loc("/conv5_1/activate/Relu_output_0_Relu_filter_reordered")
#loc50 = loc("/conv5_1/activate/Relu_output_0_Relu")
#loc51 = loc("/global_avg_pool/GlobalAveragePool_output_0_GlobalAveragePool")
#loc52 = loc("/fc/Gemm_output_0_Gemm_filter_i8")
#loc53 = loc("/fc/Gemm_output_0_Gemm_bias_int32")
#loc54 = loc("/Flatten_output_0_Flatten")
#loc55 = loc("/fc/Gemm_output_0_Gemm")

