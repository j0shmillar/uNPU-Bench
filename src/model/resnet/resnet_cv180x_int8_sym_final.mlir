#loc = loc(unknown)
#loc1 = loc("input")
module @resnet attributes {module.FLOPs = 37207396 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv180x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["input"], module.mode = "INT8", module.outputs = ["output_Reshape"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "resnet_tpu_addressed_cv180x_int8_sym_weight.npz"} {
  module @resnet attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 392992 : i64, module.device_id = 0 : i64, module.neuron_size = 57344 : i64, module.private_size = 112 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x32x32xf32> loc(unknown)) -> tensor<1x100x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", scale = [1.000000e+00, 1.000000e+00, 1.000000e+00]} : (tensor<1x3x32x32xf32>) -> tensor<1x3x32x32x!quant.uniform<i8:f32, 0.029092593700787404>, 3298534883328 : i64> loc(#loc1)
      %1 = call @subfunc_0(%0) : (tensor<1x3x32x32x!quant.uniform<i8:f32, 0.029092593700787404>, 3298534883328 : i64>) -> tensor<1x100x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> loc(#loc)
      return %1 : tensor<1x100x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x32x32x!quant.uniform<i8:f32, 0.029092593700787404>, 3298534883328 : i64> loc("input")) -> tensor<1x100x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() {do_compress = true} : () -> tensor<1x16x1x9xi8, 1099511826592 : i64> loc(#loc2)
      %2 = "top.Weight"() {do_compress = true} : () -> tensor<1x16x9x4xsi8, 1099511826736 : i64> loc(#loc3)
      %3 = "tpu.Conv2D"(%arg0, %2, %1) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x32x32x!quant.uniform<i8:f32, 0.029092593700787404>, 3298534883328 : i64>, tensor<1x16x9x4xsi8, 1099511826736 : i64>, tensor<1x16x1x9xi8, 1099511826592 : i64>) -> tensor<1x16x32x32x!quant.uniform<i8:f32, 0.019120925984251969>, 0 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x20x1x9xi8, 1099511839520 : i64> loc(#loc5)
      %5 = "top.Weight"() : () -> tensor<1x20x9x16xsi8, 1099511827504 : i64> loc(#loc6)
      %6 = "top.Weight"() : () -> tensor<1x20x1x9xi8, 1099511838736 : i64> loc(#loc7)
      %7 = "top.Weight"() : () -> tensor<1x20x9x20xsi8, 1099511839712 : i64> loc(#loc8)
      %8:2 = "tpu.Group"(%3) ({
        %40 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x32x32x!quant.uniform<i8:f32, 0.019120925984251969>, 0 : i64>) -> tensor<1x16x32x32x!quant.uniform<i8:f32, 0.019120925984251969>> loc(#loc11)
        %41 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 22528, out_size = 1440, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x9x16xsi8, 1099511827504 : i64>) -> tensor<1x20x9x16xsi8> loc(#loc12)
        %42 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12144, out_size = 90, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x1x9xi8, 1099511839520 : i64>) -> tensor<1x20x1x9xi8> loc(#loc13)
        %43 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10240, out_size = 1800, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x9x20xsi8, 1099511839712 : i64>) -> tensor<1x20x9x20xsi8> loc(#loc14)
        %44 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12048, out_size = 90, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x1x9xi8, 1099511838736 : i64>) -> tensor<1x20x1x9xi8> loc(#loc15)
        %45 = "tpu.Conv2D"(%40, %41, %42) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x32x32x!quant.uniform<i8:f32, 0.019120925984251969>>, tensor<1x20x9x16xsi8>, tensor<1x20x1x9xi8>) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>> loc(#loc9)
        %46 = "tpu.Store"(%45, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>>, none) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>, 36864 : i64> loc(#loc9)
        %47 = "tpu.Conv2D"(%45, %43, %44) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>>, tensor<1x20x9x20xsi8>, tensor<1x20x1x9xi8>) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>> loc(#loc10)
        %48 = "tpu.Store"(%47, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>>, none) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>, 16384 : i64> loc(#loc10)
        "tpu.Yield"(%46, %48) : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>, 36864 : i64>, tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>, 16384 : i64>) -> () loc(#loc80)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 8, -2, 7, 6, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x16x32x32x!quant.uniform<i8:f32, 0.019120925984251969>, 0 : i64>) -> (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>, 36864 : i64>, tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>, 16384 : i64>) loc(#loc80)
      %9 = "top.Weight"() : () -> tensor<1x20x1x9xi8, 1099511838928 : i64> loc(#loc16)
      %10 = "top.Weight"() : () -> tensor<1x20x9x20xsi8, 1099511843312 : i64> loc(#loc17)
      %11 = "tpu.Group"(%8#1, %8#0) ({
        %40 = "tpu.Load"(%8#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 5440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0, 15], h_slice = [17, 17], w_idx = [0], w_slice = [32], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>, 16384 : i64>) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>> loc(#loc19)
        %41 = "tpu.Load"(%8#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0, 15], h_slice = [17, 17], w_idx = [0], w_slice = [32], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>, 36864 : i64>) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>> loc(#loc20)
        %42 = "tpu.Load"(%10) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5440, out_size = 1800, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [20], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x9x20xsi8, 1099511843312 : i64>) -> tensor<1x20x9x20xsi8> loc(#loc21)
        %43 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7248, out_size = 90, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x1x9xi8, 1099511838928 : i64>) -> tensor<1x20x1x9xi8> loc(#loc22)
        %44 = "tpu.Add"(%40, %41) {do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5440, buffer_addr = 24576, buffer_size = 5440, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0, 15], h_slice = [17, 17], w_idx = [0], w_slice = [32], id = 4, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [52, 121], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>>, tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>>) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0068373204724409449>> loc(#loc23)
        %45 = "tpu.Conv2D"(%44, %42, %43) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0068373204724409449>>, tensor<1x20x9x20xsi8>, tensor<1x20x1x9xi8>) -> tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0029387393700787399>> loc(#loc24)
        %46 = "tpu.Pool2D"(%45) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 13632, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0, 8], h_slice = [8, 8], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0029387393700787399>>) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>> loc(#loc18)
        %47 = "tpu.Store"(%46, %0) {ginfo = #tpu.lg<out_addr = 13632, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0, 8], h_slice = [8, 8], w_idx = [0], w_slice = [16], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>>, none) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>, 0 : i64> loc(#loc18)
        "tpu.Yield"(%47) : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>, 0 : i64>) -> () loc(#loc18)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 7, -2, 5, -3, 6, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0027935464566929131>, 16384 : i64>, tensor<1x20x32x32x!quant.uniform<i8:f32, 0.0064638629921259846>, 36864 : i64>) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>, 0 : i64> loc(#loc18)
      %12 = "top.Weight"() : () -> tensor<1x20x1x9xi8, 1099511827312 : i64> loc(#loc25)
      %13 = "top.Weight"() : () -> tensor<1x20x9x20xsi8, 1099511994368 : i64> loc(#loc26)
      %14 = "top.Weight"() : () -> tensor<1x20x1x9xi8, 1099512016976 : i64> loc(#loc27)
      %15 = "top.Weight"() : () -> tensor<1x20x9x20xsi8, 1099512017168 : i64> loc(#loc28)
      %16 = "top.Weight"() : () -> tensor<1x44x1x9xi8, 1099511839120 : i64> loc(#loc29)
      %17 = "top.Weight"() : () -> tensor<1x44x9x20xsi8, 1099511830816 : i64> loc(#loc30)
      %18 = "top.Weight"() : () -> tensor<1x48x1x9xi8, 1099511830384 : i64> loc(#loc31)
      %19 = "top.Weight"() : () -> tensor<1x48x9x44xsi8, 1099511997968 : i64> loc(#loc32)
      %20 = "top.Weight"() : () -> tensor<1x48x1x9xi8, 1099511826160 : i64> loc(#loc33)
      %21 = "top.Weight"() : () -> tensor<1x48x9x48xsi8, 1099511805424 : i64> loc(#loc34)
      %22:2 = "tpu.Group"(%11) ({
        %40 = "tpu.Load"(%11) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>, 0 : i64>) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>> loc(#loc37)
        %41 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1800, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x9x20xsi8, 1099511994368 : i64>) -> tensor<1x20x9x20xsi8> loc(#loc38)
        %42 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 2560, out_size = 90, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x1x9xi8, 1099511827312 : i64>) -> tensor<1x20x1x9xi8> loc(#loc39)
        %43 = "tpu.Load"(%15) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1800, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x9x20xsi8, 1099512017168 : i64>) -> tensor<1x20x9x20xsi8> loc(#loc40)
        %44 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 90, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x20x1x9xi8, 1099512016976 : i64>) -> tensor<1x20x1x9xi8> loc(#loc41)
        %45 = "tpu.Conv2D"(%40, %41, %42) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>>, tensor<1x20x9x20xsi8>, tensor<1x20x1x9xi8>) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.001699015748031496>> loc(#loc42)
        %46 = "tpu.Conv2D"(%45, %43, %44) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.001699015748031496>>, tensor<1x20x9x20xsi8>, tensor<1x20x1x9xi8>) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0010204944881889763>> loc(#loc43)
        %47 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3960, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [44], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x44x9x20xsi8, 1099511830816 : i64>) -> tensor<1x44x9x20xsi8> loc(#loc44)
        %48 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 198, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [44], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x44x1x9xi8, 1099511839120 : i64>) -> tensor<1x44x1x9xi8> loc(#loc45)
        %49 = "tpu.Add"(%46, %45) {do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 8192, buffer_size = 2560, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [20], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [61, 102], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0010204944881889763>>, tensor<1x20x16x16x!quant.uniform<i8:f32, 0.001699015748031496>>) -> tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0021226952755905512>> loc(#loc46)
        %50 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9504, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [44], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x9x44xsi8, 1099511997968 : i64>) -> tensor<1x48x9x44xsi8> loc(#loc47)
        %51 = "tpu.Conv2D"(%49, %47, %48) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [44], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 11, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0021226952755905512>>, tensor<1x44x9x20xsi8>, tensor<1x44x1x9xi8>) -> tensor<1x44x16x16x!quant.uniform<i8:f32, 0.0013339629921259842>> loc(#loc48)
        %52 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9504, out_size = 216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x1x9xi8, 1099511830384 : i64>) -> tensor<1x48x1x9xi8> loc(#loc49)
        %53 = "tpu.Pool2D"(%51) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 24576, out_size = 1408, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [44], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 13, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x44x16x16x!quant.uniform<i8:f32, 0.0013339629921259842>>) -> tensor<1x44x8x8x!quant.uniform<i8:f32, 0.0013339629921259842>> loc(#loc50)
        %54 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 10368, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [48], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x9x48xsi8, 1099511805424 : i64>) -> tensor<1x48x9x48xsi8> loc(#loc51)
        %55 = "tpu.Load"(%20) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9728, out_size = 216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x1x9xi8, 1099511826160 : i64>) -> tensor<1x48x1x9xi8> loc(#loc52)
        %56 = "tpu.Conv2D"(%53, %50, %52) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x44x8x8x!quant.uniform<i8:f32, 0.0013339629921259842>>, tensor<1x48x9x44xsi8>, tensor<1x48x1x9xi8>) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>> loc(#loc35)
        %57 = "tpu.Store"(%56, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 17, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>>, none) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>, 8192 : i64> loc(#loc35)
        %58 = "tpu.Conv2D"(%56, %54, %55) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 10000, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>>, tensor<1x48x9x48xsi8>, tensor<1x48x1x9xi8>) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>> loc(#loc36)
        %59 = "tpu.Store"(%58, %0) {ginfo = #tpu.lg<out_addr = 10000, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 19, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>>, none) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>, 5120 : i64> loc(#loc36)
        "tpu.Yield"(%57, %59) : (tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>, 8192 : i64>, tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>, 5120 : i64>) -> () loc(#loc81)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 19, -2, 6, -3, 9, 7, 8, -4, 11, 10, -5, 13, 12, -6, 16, 14, 15, -7, 18, 17, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x20x16x16x!quant.uniform<i8:f32, 0.0029387393700787399>, 0 : i64>) -> (tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>, 8192 : i64>, tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>, 5120 : i64>) loc(#loc81)
      %23 = "top.Weight"() : () -> tensor<1x96x1x9xi8, 1099511803408 : i64> loc(#loc53)
      %24 = "top.Weight"() : () -> tensor<1x96x9x48xsi8, 1099511761936 : i64> loc(#loc54)
      %25 = "tpu.Group"(%22#1, %22#0) ({
        %40 = "tpu.Load"(%22#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>, 5120 : i64>) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>> loc(#loc56)
        %41 = "tpu.Load"(%22#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>, 8192 : i64>) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>> loc(#loc57)
        %42 = "tpu.Load"(%24) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 20736, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [48], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x9x48xsi8, 1099511761936 : i64>) -> tensor<1x96x9x48xsi8> loc(#loc58)
        %43 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23040, out_size = 432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x9xi8, 1099511803408 : i64>) -> tensor<1x96x1x9xi8> loc(#loc59)
        %44 = "tpu.Add"(%40, %41) {do_relu = false, ginfo = #tpu.lg<out_addr = 20736, out_size = 1536, buffer_addr = 26112, buffer_size = 1536, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 4, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [79, 107], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>>, tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>>) -> tensor<1x48x8x8x!quant.uniform<i8:f32, 7.8246141732283474E-4>> loc(#loc60)
        %45 = "tpu.Pool2D"(%44) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 23472, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [4], w_idx = [0], w_slice = [4], id = 5, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 7.8246141732283474E-4>>) -> tensor<1x48x4x4x!quant.uniform<i8:f32, 7.8246141732283474E-4>> loc(#loc61)
        %46 = "tpu.Conv2D"(%45, %42, %43) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 22272, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [4], w_idx = [0], w_slice = [4], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x48x4x4x!quant.uniform<i8:f32, 7.8246141732283474E-4>>, tensor<1x96x9x48xsi8>, tensor<1x96x1x9xi8>) -> tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>> loc(#loc55)
        %47 = "tpu.Store"(%46, %0) {ginfo = #tpu.lg<out_addr = 22272, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [4], w_idx = [0], w_slice = [4], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>>, none) -> tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>, 0 : i64> loc(#loc55)
        "tpu.Yield"(%47) : (tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>, 0 : i64>) -> () loc(#loc55)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 5, 2, 3, 7, -2, 6, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x48x8x8x!quant.uniform<i8:f32, 4.8420629921259843E-4>, 5120 : i64>, tensor<1x48x8x8x!quant.uniform<i8:f32, 6.5594094488188978E-4>, 8192 : i64>) -> tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>, 0 : i64> loc(#loc55)
      %26 = "top.Weight"() : () -> tensor<1x512x1x9xi8, 1099511757328 : i64> loc(#loc62)
      %27 = "top.Weight"() : () -> tensor<1x512x1x96xsi8, 1099511708176 : i64> loc(#loc63)
      %28 = "tpu.Group"(%25) ({
        %40 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31744, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [4], w_idx = [0], w_slice = [4], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>, 0 : i64>) -> tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>> loc(#loc65)
        %41 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 24576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 1, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x96xsi8, 1099511708176 : i64>) -> tensor<1x512x1x96xsi8> loc(#loc66)
        %42 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x9xi8, 1099511757328 : i64>) -> tensor<1x512x1x9xi8> loc(#loc67)
        %43 = "tpu.Pool2D"(%40) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 30976, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>>) -> tensor<1x96x2x2x!quant.uniform<i8:f32, 6.4873228346456699E-4>> loc(#loc68)
        %44 = "tpu.Conv2D"(%43, %41, %42) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x2x2x!quant.uniform<i8:f32, 6.4873228346456699E-4>>, tensor<1x512x1x96xsi8>, tensor<1x512x1x9xi8>) -> tensor<1x512x2x2x!quant.uniform<i8:f32, 9.8666062992125978E-4>> loc(#loc64)
        %45 = "tpu.Store"(%44, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 5, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x512x2x2x!quant.uniform<i8:f32, 9.8666062992125978E-4>>, none) -> tensor<1x512x2x2x!quant.uniform<i8:f32, 9.8666062992125978E-4>, 1536 : i64> loc(#loc64)
        "tpu.Yield"(%45) : (tensor<1x512x2x2x!quant.uniform<i8:f32, 9.8666062992125978E-4>, 1536 : i64>) -> () loc(#loc64)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 1, 2, 5, -2, 4, 0], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x96x4x4x!quant.uniform<i8:f32, 6.4873228346456699E-4>, 0 : i64>) -> tensor<1x512x2x2x!quant.uniform<i8:f32, 9.8666062992125978E-4>, 1536 : i64> loc(#loc64)
      %29 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099511641488 : i64> loc(#loc69)
      %30 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x512xsi8, 1099511642640 : i64> loc(#loc70)
      %31 = "tpu.Conv2D"(%28, %30, %29) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x2x2x!quant.uniform<i8:f32, 9.8666062992125978E-4>, 1536 : i64>, tensor<1x128x1x512xsi8, 1099511642640 : i64>, tensor<1x128x1x9xi8, 1099511641488 : i64>) -> tensor<1x128x2x2x!quant.uniform<i8:f32, 5.4910236220472443E-4>, 0 : i64> loc(#loc71)
      %32 = "tpu.Pool2D"(%31) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x128x2x2x!quant.uniform<i8:f32, 5.4910236220472443E-4>, 0 : i64>) -> tensor<1x128x1x1x!quant.uniform<i8:f32, 5.4910236220472443E-4>, 512 : i64> loc(#loc72)
      %33 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099511804272 : i64> loc(#loc73)
      %34 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099511846912 : i64> loc(#loc74)
      %35 = "tpu.Conv2D"(%32, %34, %33) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x1x1x!quant.uniform<i8:f32, 5.4910236220472443E-4>, 512 : i64>, tensor<1x128x9x128xsi8, 1099511846912 : i64>, tensor<1x128x1x9xi8, 1099511804272 : i64>) -> tensor<1x128x1x1x!quant.uniform<i8:f32, 2.9980787401574804E-4>, 0 : i64> loc(#loc75)
      %36 = "top.Weight"() {do_compress = true} : () -> tensor<1x100x1x9xi8, 1099511640576 : i64> loc(#loc76)
      %37 = "top.Weight"() {do_compress = true} : () -> tensor<1x100x1x128xsi8, 1099511627776 : i64> loc(#loc77)
      %38 = "tpu.Conv2D"(%35, %37, %36) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x1x1x!quant.uniform<i8:f32, 2.9980787401574804E-4>, 0 : i64>, tensor<1x100x1x128xsi8, 1099511627776 : i64>, tensor<1x100x1x9xi8, 1099511640576 : i64>) -> tensor<1x100x1x1x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> loc(#loc78)
      %39 = "tpu.Reshape"(%38) {flatten_start_dim = -1 : i64, shape = [1, -1]} : (tensor<1x100x1x1x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64>) -> tensor<1x100x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> loc(#loc79)
      return %39 : tensor<1x100x!quant.uniform<i8:f32, 5.1241574803149613E-4>, 2199023255552 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/conv1/activate/Relu_output_0_Relu_bias_packed")
#loc3 = loc("/conv1/activate/Relu_output_0_Relu_filter_reordered")
#loc4 = loc("/conv1/activate/Relu_output_0_Relu")
#loc5 = loc("/conv2/activate/Relu_output_0_Relu_bias_packed")
#loc6 = loc("/conv2/activate/Relu_output_0_Relu_filter_reordered")
#loc7 = loc("/conv3/activate/Relu_output_0_Relu_bias_packed")
#loc8 = loc("/conv3/activate/Relu_output_0_Relu_filter_reordered")
#loc9 = loc("/conv2/activate/Relu_output_0_Relu")
#loc10 = loc("/conv3/activate/Relu_output_0_Relu")
#loc11 = loc("load_/conv1/activate/Relu_output_0_Relu")
#loc12 = loc("load_/conv2/activate/Relu_output_0_Relu_filter_reordered")
#loc13 = loc("load_/conv2/activate/Relu_output_0_Relu_bias_packed")
#loc14 = loc("load_/conv3/activate/Relu_output_0_Relu_filter_reordered")
#loc15 = loc("load_/conv3/activate/Relu_output_0_Relu_bias_packed")
#loc16 = loc("/conv4/activate/Relu_output_0_Relu_bias_packed")
#loc17 = loc("/conv4/activate/Relu_output_0_Relu_filter_reordered")
#loc18 = loc("/conv5/pool/MaxPool_output_0_MaxPool")
#loc19 = loc("load_/conv3/activate/Relu_output_0_Relu")
#loc20 = loc("load_/conv2/activate/Relu_output_0_Relu")
#loc21 = loc("load_/conv4/activate/Relu_output_0_Relu_filter_reordered")
#loc22 = loc("load_/conv4/activate/Relu_output_0_Relu_bias_packed")
#loc23 = loc("/resid1/Add_output_0_Add")
#loc24 = loc("/conv4/activate/Relu_output_0_Relu")
#loc25 = loc("/conv5/activate/Relu_output_0_Relu_bias_packed")
#loc26 = loc("/conv5/activate/Relu_output_0_Relu_filter_reordered")
#loc27 = loc("/conv6/activate/Relu_output_0_Relu_bias_packed")
#loc28 = loc("/conv6/activate/Relu_output_0_Relu_filter_reordered")
#loc29 = loc("/conv7/activate/Relu_output_0_Relu_bias_packed")
#loc30 = loc("/conv7/activate/Relu_output_0_Relu_filter_reordered")
#loc31 = loc("/conv8/activate/Relu_output_0_Relu_bias_packed")
#loc32 = loc("/conv8/activate/Relu_output_0_Relu_filter_reordered")
#loc33 = loc("/conv9/activate/Relu_output_0_Relu_bias_packed")
#loc34 = loc("/conv9/activate/Relu_output_0_Relu_filter_reordered")
#loc35 = loc("/conv8/activate/Relu_output_0_Relu")
#loc36 = loc("/conv9/activate/Relu_output_0_Relu")
#loc37 = loc("load_/conv5/pool/MaxPool_output_0_MaxPool")
#loc38 = loc("load_/conv5/activate/Relu_output_0_Relu_filter_reordered")
#loc39 = loc("load_/conv5/activate/Relu_output_0_Relu_bias_packed")
#loc40 = loc("load_/conv6/activate/Relu_output_0_Relu_filter_reordered")
#loc41 = loc("load_/conv6/activate/Relu_output_0_Relu_bias_packed")
#loc42 = loc("/conv5/activate/Relu_output_0_Relu")
#loc43 = loc("/conv6/activate/Relu_output_0_Relu")
#loc44 = loc("load_/conv7/activate/Relu_output_0_Relu_filter_reordered")
#loc45 = loc("load_/conv7/activate/Relu_output_0_Relu_bias_packed")
#loc46 = loc("/resid2/Add_output_0_Add")
#loc47 = loc("load_/conv8/activate/Relu_output_0_Relu_filter_reordered")
#loc48 = loc("/conv7/activate/Relu_output_0_Relu")
#loc49 = loc("load_/conv8/activate/Relu_output_0_Relu_bias_packed")
#loc50 = loc("/conv8/pool/MaxPool_output_0_MaxPool")
#loc51 = loc("load_/conv9/activate/Relu_output_0_Relu_filter_reordered")
#loc52 = loc("load_/conv9/activate/Relu_output_0_Relu_bias_packed")
#loc53 = loc("/conv10/activate/Relu_output_0_Relu_bias_packed")
#loc54 = loc("/conv10/activate/Relu_output_0_Relu_filter_reordered")
#loc55 = loc("/conv10/activate/Relu_output_0_Relu")
#loc56 = loc("load_/conv9/activate/Relu_output_0_Relu")
#loc57 = loc("load_/conv8/activate/Relu_output_0_Relu")
#loc58 = loc("load_/conv10/activate/Relu_output_0_Relu_filter_reordered")
#loc59 = loc("load_/conv10/activate/Relu_output_0_Relu_bias_packed")
#loc60 = loc("/resid3/Add_output_0_Add")
#loc61 = loc("/conv10/pool/MaxPool_output_0_MaxPool")
#loc62 = loc("/conv11/activate/Relu_output_0_Relu_bias_packed")
#loc63 = loc("/conv11/activate/Relu_output_0_Relu_filter_reordered")
#loc64 = loc("/conv11/activate/Relu_output_0_Relu")
#loc65 = loc("load_/conv10/activate/Relu_output_0_Relu")
#loc66 = loc("load_/conv11/activate/Relu_output_0_Relu_filter_reordered")
#loc67 = loc("load_/conv11/activate/Relu_output_0_Relu_bias_packed")
#loc68 = loc("/conv11/pool/MaxPool_output_0_MaxPool")
#loc69 = loc("/conv12/activate/Relu_output_0_Relu_bias_packed")
#loc70 = loc("/conv12/activate/Relu_output_0_Relu_filter_reordered")
#loc71 = loc("/conv12/activate/Relu_output_0_Relu")
#loc72 = loc("/conv13/pool/MaxPool_output_0_MaxPool")
#loc73 = loc("/conv13/activate/Relu_output_0_Relu_bias_packed")
#loc74 = loc("/conv13/activate/Relu_output_0_Relu_filter_reordered")
#loc75 = loc("/conv13/activate/Relu_output_0_Relu")
#loc76 = loc("/conv14/Conv_output_0_Conv_bias_packed")
#loc77 = loc("/conv14/Conv_output_0_Conv_filter_reordered")
#loc78 = loc("/conv14/Conv_output_0_Conv")
#loc79 = loc("output_Reshape")
#loc80 = loc(fused[#loc9, #loc10])
#loc81 = loc(fused[#loc35, #loc36])

