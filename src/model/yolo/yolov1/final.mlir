#loc = loc(unknown)
#loc1 = loc("input")
module @yolov1 attributes {module.FLOPs = 43289280 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv180x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["input"], module.mode = "INT8", module.outputs = ["output_Slice", "667_Slice", "/Conv_Res_4/Conv_output_0_Conv"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "yolov1_tpu_addressed_cv180x_int8_sym_weight.npz"} {
  module @yolov1 attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 44256 : i64, module.device_id = 0 : i64, module.neuron_size = 46080 : i64, module.private_size = 0 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x96x96xf32> loc(unknown)) -> (tensor<1x12x12x10x!quant.uniform<i8:f32, 0.0022237023622047243>, 5497558138880 : i64>, tensor<1x12x12x2x!quant.uniform<i8:f32, 0.0022237023622047243>, 6597069766656 : i64>, tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64>) {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", scale = [1.000000e+00, 1.000000e+00, 1.000000e+00]} : (tensor<1x3x96x96xf32>) -> tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>, 3298534883328 : i64> loc(#loc1)
      %1:3 = call @subfunc_0(%0) : (tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>, 3298534883328 : i64>) -> (tensor<1x12x12x10x!quant.uniform<i8:f32, 0.0022237023622047243>, 5497558138880 : i64>, tensor<1x12x12x2x!quant.uniform<i8:f32, 0.0022237023622047243>, 6597069766656 : i64>, tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64>) loc(#loc)
      return %1#0, %1#1, %1#2 : tensor<1x12x12x10x!quant.uniform<i8:f32, 0.0022237023622047243>, 5497558138880 : i64>, tensor<1x12x12x2x!quant.uniform<i8:f32, 0.0022237023622047243>, 6597069766656 : i64>, tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>, 3298534883328 : i64> loc("input")) -> (tensor<1x12x12x10x!quant.uniform<i8:f32, 0.0022237023622047243>, 5497558138880 : i64>, tensor<1x12x12x2x!quant.uniform<i8:f32, 0.0022237023622047243>, 6597069766656 : i64>, tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x16x1x5xi8, 1099511649232 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x16x9x4xsi8, 1099511648320 : i64> loc(#loc3)
      %3 = "tpu.Group"(%arg0) ({
        %47 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4992, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 23, 47, 71], h_slice = [25, 26, 26, 25], w_idx = [0], w_slice = [96], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 4 : i64} : (tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>, 3298534883328 : i64>) -> tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>> loc(#loc5)
        %48 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x4xsi8, 1099511648320 : i64>) -> tensor<1x16x9x4xsi8> loc(#loc6)
        %49 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18720, out_size = 40, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x5xi8, 1099511649232 : i64>) -> tensor<1x16x1x5xi8> loc(#loc7)
        %50 = "tpu.Conv2D"(%47, %48, %49) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 24, 48, 72], h_slice = [24, 24, 24, 24], w_idx = [0], w_slice = [96], id = 3, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>>, tensor<1x16x9x4xsi8>, tensor<1x16x1x5xi8>) -> tensor<1x16x96x96x!quant.uniform<i8:f32, 0.0200665125984252>> loc(#loc8)
        %51 = "tpu.Pool2D"(%50) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 25472, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 12, 24, 36], h_slice = [12, 12, 12, 12], w_idx = [0], w_slice = [48], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x16x96x96x!quant.uniform<i8:f32, 0.0200665125984252>>) -> tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>> loc(#loc4)
        %52 = "tpu.Store"(%51, %0) {ginfo = #tpu.lg<out_addr = 25472, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 12, 24, 36], h_slice = [12, 12, 12, 12], w_idx = [0], w_slice = [48], id = 5, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>>, none) -> tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>, 0 : i64> loc(#loc4)
        "tpu.Yield"(%52) : (tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>, 0 : i64>) -> () loc(#loc4)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 5, -2, 4, 0, 1, 2], group_type = 0 : i64, hsecs = 4 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x96x96x!quant.uniform<i8:f32, 0.033477193700787403>, 3298534883328 : i64>) -> tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>, 0 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x16x1x5xi8, 1099511648896 : i64> loc(#loc9)
      %5 = "top.Weight"() : () -> tensor<1x16x9x16xsi8, 1099511646016 : i64> loc(#loc10)
      %6 = "tpu.Group"(%3) ({
        %47 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 23], h_slice = [25, 25], w_idx = [0], w_slice = [48], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>, 0 : i64>) -> tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>> loc(#loc12)
        %48 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x16xsi8, 1099511646016 : i64>) -> tensor<1x16x9x16xsi8> loc(#loc13)
        %49 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9600, out_size = 40, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x5xi8, 1099511648896 : i64>) -> tensor<1x16x1x5xi8> loc(#loc14)
        %50 = "tpu.Conv2D"(%47, %48, %49) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 24], h_slice = [24, 24], w_idx = [0], w_slice = [48], id = 3, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>>, tensor<1x16x9x16xsi8>, tensor<1x16x1x5xi8>) -> tensor<1x16x48x48x!quant.uniform<i8:f32, 0.011605399212598426>> loc(#loc15)
        %51 = "tpu.Pool2D"(%50) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 12], h_slice = [12, 12], w_idx = [0], w_slice = [24], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x16x48x48x!quant.uniform<i8:f32, 0.011605399212598426>>) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>> loc(#loc11)
        %52 = "tpu.Store"(%51, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 12], h_slice = [12, 12], w_idx = [0], w_slice = [24], id = 5, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>>, none) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>, 36864 : i64> loc(#loc11)
        "tpu.Yield"(%52) : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>, 36864 : i64>) -> () loc(#loc11)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 5, -2, 4, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x16x48x48x!quant.uniform<i8:f32, 0.0200665125984252>, 0 : i64>) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>, 36864 : i64> loc(#loc11)
      %7 = "top.Weight"() : () -> tensor<1x16x1x5xi8, 1099511630576 : i64> loc(#loc16)
      %8 = "top.Weight"() : () -> tensor<1x16x1x16xsi8, 1099511645472 : i64> loc(#loc17)
      %9 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511630144 : i64> loc(#loc18)
      %10 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511630800 : i64> loc(#loc19)
      %11 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511630432 : i64> loc(#loc20)
      %12 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511660416 : i64> loc(#loc21)
      %13 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511629856 : i64> loc(#loc22)
      %14 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511649312 : i64> loc(#loc23)
      %15 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511640720 : i64> loc(#loc24)
      %16 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511635408 : i64> loc(#loc25)
      %17 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511645728 : i64> loc(#loc26)
      %18 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511635920 : i64> loc(#loc27)
      %19 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511648976 : i64> loc(#loc28)
      %20 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511653920 : i64> loc(#loc29)
      %21 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511659616 : i64> loc(#loc30)
      %22 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511660928 : i64> loc(#loc31)
      %23 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511630656 : i64> loc(#loc32)
      %24 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511665536 : i64> loc(#loc33)
      %25 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511666336 : i64> loc(#loc34)
      %26 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511640864 : i64> loc(#loc35)
      %27 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511667136 : i64> loc(#loc36)
      %28 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511666624 : i64> loc(#loc37)
      %29 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511654720 : i64> loc(#loc38)
      %30 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511667280 : i64> loc(#loc39)
      %31 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511671888 : i64> loc(#loc40)
      %32 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511629344 : i64> loc(#loc41)
      %33 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511666048 : i64> loc(#loc42)
      %34 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099511655008 : i64> loc(#loc43)
      %35 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099511654432 : i64> loc(#loc44)
      %36 = "top.Weight"() : () -> tensor<1x32x1x32xsi8, 1099511628320 : i64> loc(#loc45)
      %37 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511628176 : i64> loc(#loc46)
      %38 = "top.Weight"() : () -> tensor<1x16x1x32xsi8, 1099511659904 : i64> loc(#loc47)
      %39 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099511628032 : i64> loc(#loc48)
      %40 = "top.Weight"() : () -> tensor<1x16x1x16xsi8, 1099511627776 : i64> loc(#loc49)
      %41 = "top.Weight"() : () -> tensor<1x12x1x9xi8, 1099511649120 : i64> loc(#loc50)
      %42 = "top.Weight"() : () -> tensor<1x12x1x16xsi8, 1099511640528 : i64> loc(#loc51)
      %43 = "tpu.Group"(%6) ({
        %47 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>, 36864 : i64>) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>> loc(#loc53)
        %48 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x16xsi8, 1099511645472 : i64>) -> tensor<1x16x1x16xsi8> loc(#loc54)
        %49 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 40, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x5xi8, 1099511630576 : i64>) -> tensor<1x16x1x5xi8> loc(#loc55)
        %50 = "tpu.Load"(%10) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511630800 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc56)
        %51 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511630144 : i64>) -> tensor<1x32x1x9xi8> loc(#loc57)
        %52 = "tpu.Conv2D"(%47, %48, %49) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>>, tensor<1x16x1x16xsi8>, tensor<1x16x1x5xi8>) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.004694941732283465>> loc(#loc58)
        %53 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511660416 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc59)
        %54 = "tpu.Load"(%11) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511630432 : i64>) -> tensor<1x16x1x9xi8> loc(#loc60)
        %55 = "tpu.Conv2D"(%52, %50, %51) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.004694941732283465>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x24x24x!quant.uniform<i8:f32, 0.0019906858267716536>> loc(#loc61)
        %56 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511649312 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc62)
        %57 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511629856 : i64>) -> tensor<1x32x1x9xi8> loc(#loc63)
        %58 = "tpu.Conv2D"(%55, %53, %54) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 11, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x24x24x!quant.uniform<i8:f32, 0.0019906858267716536>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.0020158385826771655>> loc(#loc64)
        %59 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511635408 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc65)
        %60 = "tpu.Load"(%15) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 13, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511640720 : i64>) -> tensor<1x16x1x9xi8> loc(#loc66)
        %61 = "tpu.Conv2D"(%58, %56, %57) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 14, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.0020158385826771655>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x24x24x!quant.uniform<i8:f32, 0.0011684055118110236>> loc(#loc67)
        %62 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511635920 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc68)
        %63 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 16, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511645728 : i64>) -> tensor<1x32x1x9xi8> loc(#loc69)
        %64 = "tpu.Conv2D"(%61, %59, %60) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 17, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x24x24x!quant.uniform<i8:f32, 0.0011684055118110236>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x24x24x!quant.uniform<i8:f32, 0.001282120472440945>> loc(#loc70)
        %65 = "tpu.Conv2D"(%64, %62, %63) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [24], w_idx = [0], w_slice = [24], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.001282120472440945>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x24x24x!quant.uniform<i8:f32, 0.0010661700787401576>> loc(#loc71)
        %66 = "tpu.Load"(%20) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 19, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511653920 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc72)
        %67 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 20, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511648976 : i64>) -> tensor<1x16x1x9xi8> loc(#loc73)
        %68 = "tpu.Pool2D"(%65) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 21, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x32x24x24x!quant.uniform<i8:f32, 0.0010661700787401576>>) -> tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0010661700787401576>> loc(#loc74)
        %69 = "tpu.Load"(%22) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 22, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511660928 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc75)
        %70 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 23, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511659616 : i64>) -> tensor<1x32x1x9xi8> loc(#loc76)
        %71 = "tpu.Conv2D"(%68, %66, %67) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 24, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0010661700787401576>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x12x12x!quant.uniform<i8:f32, 0.001602588188976378>> loc(#loc77)
        %72 = "tpu.Load"(%24) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 25, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511665536 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc78)
        %73 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 26, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511630656 : i64>) -> tensor<1x16x1x9xi8> loc(#loc79)
        %74 = "tpu.Conv2D"(%71, %69, %70) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 27, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x12x12x!quant.uniform<i8:f32, 0.001602588188976378>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0015337653543307085>> loc(#loc80)
        %75 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 28, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511640864 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc81)
        %76 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 29, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511666336 : i64>) -> tensor<1x32x1x9xi8> loc(#loc82)
        %77 = "tpu.Conv2D"(%74, %72, %73) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 30, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0015337653543307085>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0012965456692913386>> loc(#loc83)
        %78 = "tpu.Load"(%28) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 31, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511666624 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc84)
        %79 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 32, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511667136 : i64>) -> tensor<1x16x1x9xi8> loc(#loc85)
        %80 = "tpu.Conv2D"(%77, %75, %76) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 33, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0012965456692913386>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x12x12x!quant.uniform<i8:f32, 8.4408031496062982E-4>> loc(#loc86)
        %81 = "tpu.Load"(%30) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 34, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511667280 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc87)
        %82 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 35, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511654720 : i64>) -> tensor<1x32x1x9xi8> loc(#loc88)
        %83 = "tpu.Conv2D"(%80, %78, %79) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 36, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x12x12x!quant.uniform<i8:f32, 8.4408031496062982E-4>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0015466204724409449>> loc(#loc89)
        %84 = "tpu.Load"(%32) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 37, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511629344 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc90)
        %85 = "tpu.Load"(%31) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 38, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511671888 : i64>) -> tensor<1x16x1x9xi8> loc(#loc91)
        %86 = "tpu.Conv2D"(%83, %81, %82) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 39, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0015466204724409449>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0011808425196850392>> loc(#loc92)
        %87 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 40, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099511655008 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc93)
        %88 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 41, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511666048 : i64>) -> tensor<1x32x1x9xi8> loc(#loc94)
        %89 = "tpu.Conv2D"(%86, %84, %85) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 42, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0011808425196850392>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0015716236220472441>> loc(#loc95)
        %90 = "tpu.Load"(%36) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 43, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x32xsi8, 1099511628320 : i64>) -> tensor<1x32x1x32xsi8> loc(#loc96)
        %91 = "tpu.Load"(%35) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 44, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099511654432 : i64>) -> tensor<1x32x1x9xi8> loc(#loc97)
        %92 = "tpu.Conv2D"(%89, %87, %88) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 45, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0015716236220472441>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0011358724409448817>> loc(#loc98)
        %93 = "tpu.Load"(%38) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 46, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x32xsi8, 1099511659904 : i64>) -> tensor<1x16x1x32xsi8> loc(#loc99)
        %94 = "tpu.Load"(%37) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 47, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511628176 : i64>) -> tensor<1x16x1x9xi8> loc(#loc100)
        %95 = "tpu.Conv2D"(%92, %90, %91) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 48, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0011358724409448817>>, tensor<1x32x1x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0016324755905511811>> loc(#loc101)
        %96 = "tpu.Load"(%40) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [16], id = 49, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x16xsi8, 1099511627776 : i64>) -> tensor<1x16x1x16xsi8> loc(#loc102)
        %97 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 50, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099511628032 : i64>) -> tensor<1x16x1x9xi8> loc(#loc103)
        %98 = "tpu.Conv2D"(%95, %93, %94) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 51, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x12x12x!quant.uniform<i8:f32, 0.0016324755905511811>>, tensor<1x16x1x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0011609307086614172>> loc(#loc104)
        %99 = "tpu.Load"(%42) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 96, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [12], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [16], id = 52, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x12x1x16xsi8, 1099511640528 : i64>) -> tensor<1x12x1x16xsi8> loc(#loc105)
        %100 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 54, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [12], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 53, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x12x1x9xi8, 1099511649120 : i64>) -> tensor<1x12x1x9xi8> loc(#loc106)
        %101 = "tpu.Conv2D"(%98, %96, %97) {coeff_merged = false, dilations = [1, 1], do_relu = true, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 54, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0011609307086614172>>, tensor<1x16x1x16xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0024052543307086613>> loc(#loc107)
        %102 = "tpu.Conv2D"(%101, %99, %100) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 22784, out_size = 864, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [12], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 55, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x12x12x!quant.uniform<i8:f32, 0.0024052543307086613>>, tensor<1x12x1x16xsi8>, tensor<1x12x1x9xi8>) -> tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>> loc(#loc52)
        %103 = "tpu.Store"(%102, %0) {ginfo = #tpu.lg<out_addr = 22784, out_size = 864, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [12], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [12], w_idx = [0], w_slice = [12], id = 56, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>>, none) -> tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64> loc(#loc52)
        "tpu.Yield"(%103) : (tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64>) -> () loc(#loc52)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 56, -2, 8, 6, 7, -3, 11, 9, 10, -4, 14, 12, 13, -5, 17, 15, 16, -6, 18, -7, 21, 19, 20, -8, 24, 22, 23, -9, 27, 25, 26, -10, 30, 28, 29, -11, 33, 31, 32, -12, 36, 34, 35, -13, 39, 37, 38, -14, 42, 40, 41, -15, 45, 43, 44, -16, 48, 46, 47, -17, 51, 49, 50, -18, 54, 52, 53, -19, 55, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x16x24x24x!quant.uniform<i8:f32, 0.011605399212598426>, 36864 : i64>) -> tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64> loc(#loc52)
      %44 = "tpu.Permute"(%43, %0) {order = [0, 2, 3, 1]} : (tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64>, none) -> tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 0 : i64> loc(#loc108)
      %45 = "tpu.Slice"(%44, %0, %0, %0, %0) {axes = [], ends = [1, 12, 12, 10], hasparamConvert_axes = [3], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 0 : i64>, none, none, none, none) -> tensor<1x12x12x10x!quant.uniform<i8:f32, 0.0022237023622047243>, 5497558138880 : i64> loc(#loc109)
      %46 = "tpu.Slice"(%44, %0, %0, %0, %0) {axes = [], ends = [1, 12, 12, 2147482624], hasparamConvert_axes = [3], offset = [0, 0, 0, 10], steps = [1, 1, 1, 1]} : (tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 0 : i64>, none, none, none, none) -> tensor<1x12x12x2x!quant.uniform<i8:f32, 0.0022237023622047243>, 6597069766656 : i64> loc(#loc110)
      return %45, %46, %43 : tensor<1x12x12x10x!quant.uniform<i8:f32, 0.0022237023622047243>, 5497558138880 : i64>, tensor<1x12x12x2x!quant.uniform<i8:f32, 0.0022237023622047243>, 6597069766656 : i64>, tensor<1x12x12x12x!quant.uniform<i8:f32, 0.0022237023622047243>, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/Conv_96/activate/Relu_output_0_Relu_bias_packed")
#loc3 = loc("/Conv_96/activate/Relu_output_0_Relu_filter_reordered")
#loc4 = loc("/Conv_48/pool/MaxPool_output_0_MaxPool")
#loc5 = loc("load_0")
#loc6 = loc("load_/Conv_96/activate/Relu_output_0_Relu_filter_reordered")
#loc7 = loc("load_/Conv_96/activate/Relu_output_0_Relu_bias_packed")
#loc8 = loc("/Conv_96/activate/Relu_output_0_Relu")
#loc9 = loc("/Conv_48/activate/Relu_output_0_Relu_bias_packed")
#loc10 = loc("/Conv_48/activate/Relu_output_0_Relu_filter_reordered")
#loc11 = loc("/Conv_24_1/pool/MaxPool_output_0_MaxPool")
#loc12 = loc("load_/Conv_48/pool/MaxPool_output_0_MaxPool")
#loc13 = loc("load_/Conv_48/activate/Relu_output_0_Relu_filter_reordered")
#loc14 = loc("load_/Conv_48/activate/Relu_output_0_Relu_bias_packed")
#loc15 = loc("/Conv_48/activate/Relu_output_0_Relu")
#loc16 = loc("/Conv_24_1/activate/Relu_output_0_Relu_bias_packed")
#loc17 = loc("/Conv_24_1/activate/Relu_output_0_Relu_filter_reordered")
#loc18 = loc("/Conv_24_2/activate/Relu_output_0_Relu_bias_packed")
#loc19 = loc("/Conv_24_2/activate/Relu_output_0_Relu_filter_reordered")
#loc20 = loc("/Conv_24_3/activate/Relu_output_0_Relu_bias_packed")
#loc21 = loc("/Conv_24_3/activate/Relu_output_0_Relu_filter_reordered")
#loc22 = loc("/Conv_24_4/activate/Relu_output_0_Relu_bias_packed")
#loc23 = loc("/Conv_24_4/activate/Relu_output_0_Relu_filter_reordered")
#loc24 = loc("/Conv_24_5/activate/Relu_output_0_Relu_bias_packed")
#loc25 = loc("/Conv_24_5/activate/Relu_output_0_Relu_filter_reordered")
#loc26 = loc("/Conv_24_6/activate/Relu_output_0_Relu_bias_packed")
#loc27 = loc("/Conv_24_6/activate/Relu_output_0_Relu_filter_reordered")
#loc28 = loc("/Conv_12_1/activate/Relu_output_0_Relu_bias_packed")
#loc29 = loc("/Conv_12_1/activate/Relu_output_0_Relu_filter_reordered")
#loc30 = loc("/Conv_12_2/activate/Relu_output_0_Relu_bias_packed")
#loc31 = loc("/Conv_12_2/activate/Relu_output_0_Relu_filter_reordered")
#loc32 = loc("/Conv_12_3/activate/Relu_output_0_Relu_bias_packed")
#loc33 = loc("/Conv_12_3/activate/Relu_output_0_Relu_filter_reordered")
#loc34 = loc("/Conv_12_4/activate/Relu_output_0_Relu_bias_packed")
#loc35 = loc("/Conv_12_4/activate/Relu_output_0_Relu_filter_reordered")
#loc36 = loc("/Conv_12_5/activate/Relu_output_0_Relu_bias_packed")
#loc37 = loc("/Conv_12_5/activate/Relu_output_0_Relu_filter_reordered")
#loc38 = loc("/Conv_12_6/activate/Relu_output_0_Relu_bias_packed")
#loc39 = loc("/Conv_12_6/activate/Relu_output_0_Relu_filter_reordered")
#loc40 = loc("/Conv_7_1/activate/Relu_output_0_Relu_bias_packed")
#loc41 = loc("/Conv_7_1/activate/Relu_output_0_Relu_filter_reordered")
#loc42 = loc("/Conv_7_2/activate/Relu_output_0_Relu_bias_packed")
#loc43 = loc("/Conv_7_2/activate/Relu_output_0_Relu_filter_reordered")
#loc44 = loc("/Conv_Res_1/activate/Relu_output_0_Relu_bias_packed")
#loc45 = loc("/Conv_Res_1/activate/Relu_output_0_Relu_filter_reordered")
#loc46 = loc("/Conv_Res_2/activate/Relu_output_0_Relu_bias_packed")
#loc47 = loc("/Conv_Res_2/activate/Relu_output_0_Relu_filter_reordered")
#loc48 = loc("/Conv_Res_3/activate/Relu_output_0_Relu_bias_packed")
#loc49 = loc("/Conv_Res_3/activate/Relu_output_0_Relu_filter_reordered")
#loc50 = loc("/Conv_Res_4/Conv_output_0_Conv_bias_packed")
#loc51 = loc("/Conv_Res_4/Conv_output_0_Conv_filter_reordered")
#loc52 = loc("/Conv_Res_4/Conv_output_0_Conv")
#loc53 = loc("load_/Conv_24_1/pool/MaxPool_output_0_MaxPool")
#loc54 = loc("load_/Conv_24_1/activate/Relu_output_0_Relu_filter_reordered")
#loc55 = loc("load_/Conv_24_1/activate/Relu_output_0_Relu_bias_packed")
#loc56 = loc("load_/Conv_24_2/activate/Relu_output_0_Relu_filter_reordered")
#loc57 = loc("load_/Conv_24_2/activate/Relu_output_0_Relu_bias_packed")
#loc58 = loc("/Conv_24_1/activate/Relu_output_0_Relu")
#loc59 = loc("load_/Conv_24_3/activate/Relu_output_0_Relu_filter_reordered")
#loc60 = loc("load_/Conv_24_3/activate/Relu_output_0_Relu_bias_packed")
#loc61 = loc("/Conv_24_2/activate/Relu_output_0_Relu")
#loc62 = loc("load_/Conv_24_4/activate/Relu_output_0_Relu_filter_reordered")
#loc63 = loc("load_/Conv_24_4/activate/Relu_output_0_Relu_bias_packed")
#loc64 = loc("/Conv_24_3/activate/Relu_output_0_Relu")
#loc65 = loc("load_/Conv_24_5/activate/Relu_output_0_Relu_filter_reordered")
#loc66 = loc("load_/Conv_24_5/activate/Relu_output_0_Relu_bias_packed")
#loc67 = loc("/Conv_24_4/activate/Relu_output_0_Relu")
#loc68 = loc("load_/Conv_24_6/activate/Relu_output_0_Relu_filter_reordered")
#loc69 = loc("load_/Conv_24_6/activate/Relu_output_0_Relu_bias_packed")
#loc70 = loc("/Conv_24_5/activate/Relu_output_0_Relu")
#loc71 = loc("/Conv_24_6/activate/Relu_output_0_Relu")
#loc72 = loc("load_/Conv_12_1/activate/Relu_output_0_Relu_filter_reordered")
#loc73 = loc("load_/Conv_12_1/activate/Relu_output_0_Relu_bias_packed")
#loc74 = loc("/Conv_12_1/pool/MaxPool_output_0_MaxPool")
#loc75 = loc("load_/Conv_12_2/activate/Relu_output_0_Relu_filter_reordered")
#loc76 = loc("load_/Conv_12_2/activate/Relu_output_0_Relu_bias_packed")
#loc77 = loc("/Conv_12_1/activate/Relu_output_0_Relu")
#loc78 = loc("load_/Conv_12_3/activate/Relu_output_0_Relu_filter_reordered")
#loc79 = loc("load_/Conv_12_3/activate/Relu_output_0_Relu_bias_packed")
#loc80 = loc("/Conv_12_2/activate/Relu_output_0_Relu")
#loc81 = loc("load_/Conv_12_4/activate/Relu_output_0_Relu_filter_reordered")
#loc82 = loc("load_/Conv_12_4/activate/Relu_output_0_Relu_bias_packed")
#loc83 = loc("/Conv_12_3/activate/Relu_output_0_Relu")
#loc84 = loc("load_/Conv_12_5/activate/Relu_output_0_Relu_filter_reordered")
#loc85 = loc("load_/Conv_12_5/activate/Relu_output_0_Relu_bias_packed")
#loc86 = loc("/Conv_12_4/activate/Relu_output_0_Relu")
#loc87 = loc("load_/Conv_12_6/activate/Relu_output_0_Relu_filter_reordered")
#loc88 = loc("load_/Conv_12_6/activate/Relu_output_0_Relu_bias_packed")
#loc89 = loc("/Conv_12_5/activate/Relu_output_0_Relu")
#loc90 = loc("load_/Conv_7_1/activate/Relu_output_0_Relu_filter_reordered")
#loc91 = loc("load_/Conv_7_1/activate/Relu_output_0_Relu_bias_packed")
#loc92 = loc("/Conv_12_6/activate/Relu_output_0_Relu")
#loc93 = loc("load_/Conv_7_2/activate/Relu_output_0_Relu_filter_reordered")
#loc94 = loc("load_/Conv_7_2/activate/Relu_output_0_Relu_bias_packed")
#loc95 = loc("/Conv_7_1/activate/Relu_output_0_Relu")
#loc96 = loc("load_/Conv_Res_1/activate/Relu_output_0_Relu_filter_reordered")
#loc97 = loc("load_/Conv_Res_1/activate/Relu_output_0_Relu_bias_packed")
#loc98 = loc("/Conv_7_2/activate/Relu_output_0_Relu")
#loc99 = loc("load_/Conv_Res_2/activate/Relu_output_0_Relu_filter_reordered")
#loc100 = loc("load_/Conv_Res_2/activate/Relu_output_0_Relu_bias_packed")
#loc101 = loc("/Conv_Res_1/activate/Relu_output_0_Relu")
#loc102 = loc("load_/Conv_Res_3/activate/Relu_output_0_Relu_filter_reordered")
#loc103 = loc("load_/Conv_Res_3/activate/Relu_output_0_Relu_bias_packed")
#loc104 = loc("/Conv_Res_2/activate/Relu_output_0_Relu")
#loc105 = loc("load_/Conv_Res_4/Conv_output_0_Conv_filter_reordered")
#loc106 = loc("load_/Conv_Res_4/Conv_output_0_Conv_bias_packed")
#loc107 = loc("/Conv_Res_3/activate/Relu_output_0_Relu")
#loc108 = loc("/Transpose_output_0_Transpose")
#loc109 = loc("output_Slice")
#loc110 = loc("667_Slice")

