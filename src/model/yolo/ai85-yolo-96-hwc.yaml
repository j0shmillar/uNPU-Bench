# CHW (big data) configuration for MNIST

arch: Yolov1_net
dataset: yolov1

layers:
  #self.Conv_96
  - pad: 1
    activate: ReLU
    out_offset: 0x2000
    processors: 0x0000000000000007
    # FusedConv2dReLU(in_channels=3, out_channels=16, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs)
    data_format: HWC
    op: conv2d
    kernel_size: 3x3
    streaming: true
    quantization: 8

  #self.Conv_48
  - pad: 1
    activate: ReLU
    out_offset: 0x3000
    processors: 0x000000000000ffff
    # FusedMaxPoolConv2dReLU(in_channels=16, out_channels=16, kernel_size=3,
    #                        stride=1, padding=1, bias=bias, **kwargs)
    op: conv2d
    kernel_size: 3x3
    streaming: true
    max_pool: 2
    pool_stride: 2
    quantization: 8

  #self.Conv_24
  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0xffff000000000000
    # FusedMaxPoolConv2dReLU(in_channels=16, out_channels=16, kernel_size=1,
    #                        stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    streaming: true
    max_pool: 2
    pool_stride: 2
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0x000000000000ffff
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
  #  streaming: true
    quantization: 8

  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0x00000000ffffffff
    # FusedConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
  #  streaming: true
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0xffff000000000000
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
  #  streaming: true
    quantization: 8

  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0xffffffff00000000
    # FusedConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
  #  streaming: true
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0x000000000000ffff
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
  #  streaming: true
    quantization: 8

  #self.Conv_12
  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0x00000000ffffffff
    # FusedMaxPoolConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                        stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    max_pool: 2
    pool_stride: 2
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0xffff000000000000
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
    quantization: 8

  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0xffffffff00000000
    # FusedConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0x000000000000ffff
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
    quantization: 8

  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0x00000000ffffffff
    # FusedConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0xffff000000000000
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
    quantization: 8


  #self.Conv_7
  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0xffffffff00000000
    # FusedConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    quantization: 8

  - pad: 1
    activate: ReLU
    out_offset: 0x0000
    processors: 0x000000000000ffff
    # FusedConv2dReLU(in_channels=16, out_channels=32, kernel_size=3,
    #                 stride=1, padding=1, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 3x3
    quantization: 8

  #self.Conv_Res
  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0x00000000ffffffff
    # FusedConv2dReLU(in_channels=32, out_channels=32, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    quantization: 8

  - pad: 0
    activate: ReLU
    out_offset: 0x0000
    processors: 0xffffffff00000000
    # FusedConv2dReLU(in_channels=32, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    quantization: 8

  - pad: 0
    activate: ReLU
    out_offset: 0x4000
    processors: 0xffff000000000000
    # FusedConv2dReLU(in_channels=16, out_channels=16, kernel_size=1,
    #                 stride=1, padding=0, bias=bias, **kwargs),
    op: conv2d
    kernel_size: 1x1
    quantization: 8

  - pad: 0
    out_offset: 0x0000
    processors: 0x000000000000ffff
    # Conv2d(in_channels=16,
    #        out_channels=self.B * 5 + self.Classes_Num, kernel_size=1,
    #        stride=1, padding=0, bias=True, wide=True, **kwargs)
    op: conv2d
    kernel_size: 1x1
    output_width: 32
    quantization: 8
